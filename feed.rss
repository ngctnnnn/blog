<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>A technical blog</title>
        <link>https://blog.vuejs.org</link>
        <description>The offical technical blog by Tan Ngoc Pham</description>
        <lastBuildDate>Mon, 14 Feb 2022 04:11:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <image>
            <title>A technical blog</title>
            <url>https://vuejs.org/images/logo.png</url>
            <link>https://blog.vuejs.org</link>
        </image>
        <item>
            <title><![CDATA[An introduction to Reinforcement Learning]]></title>
            <link>https://blog.vuejs.org/posts/introduction-to-reinforcement-learning.html</link>
            <guid>https://blog.vuejs.org/posts/introduction-to-reinforcement-learning.html</guid>
            <pubDate>Sun, 13 Feb 2022 12:00:00 GMT</pubDate>
            <description><![CDATA[ 
Besides traditional data-based methods on Machine Learning, e.g. Clustering or Maximum Likelihood Estimation, Reinforcement Learning is a family in which tiny (or even no) data is required to do the training and testing phase. In this post, I would give a minor introduction on Reinforcement Learning, its basic concepts and methods.

]]></description>
            <content:encoded><![CDATA[<div><p>Besides traditional data-based methods on Machine Learning, e.g. Clustering or Maximum Likelihood Estimation, Reinforcement Learning is a family in which tiny (or even no) data is required to do the training and testing phase. In this post, I would give a minor introduction on Reinforcement Learning, its basic concepts and methods.</p><hr><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h2><ul><li><ol><li><a href="#:~:text=1.%20What%20is%20Reinforcement%20Learning">Introduction</a></li></ol></li><li><ol start="2"><li><a href="#:~:text=2.%20Principal%20(mathematical)%20concepts%20of%20Reinforcement%20Learning">Principal (mathematical) concepts of Reinforcement Learning</a></li></ol><ul><li>a. <a href="#:~:text=a.%20Transition%20and%20Reward">Transition &amp; Reward</a></li><li>b. <a href="#:~:text=b.%20Policy">Stochastic &amp; Deterministic Policy</a></li><li>c. <a href="#:~:text=c.%20State%2DValue%20and%20Action%2DValue%20Function">State-Value &amp; Action-Value Function</a></li></ul></li><li><ol start="3"><li><a href="#:~:text=3.%20A%20literature%20on%20preliminary%20approaches%20in%25Reinforcement%20Learning">A literature on preliminary approaches in Reinforcement Learning</a></li></ol><ul><li>a. <a href="#:~:text=a.%20Dynamic%20Programming">Dynamic Programming</a></li><li>b. <a href="#:~:text=b.%20Q%2DLearning">Q-Learning</a></li></ul></li><li><ol start="4"><li><a href="#:~:text=4.%20Conclusions">Conclusions</a></li></ol></li></ul><hr><h2 id="_1-what-is-reinforcement-learning" tabindex="-1">1. What is Reinforcement Learning <a class="header-anchor" href="#_1-what-is-reinforcement-learning" aria-hidden="true">#</a></h2><p>Let say you want to help Pac-man on the game of Pac-man achieve <ins>the best score</ins> among <ins>different rounds and difficulties</ins>. However, you do not know surely how to play the game optimally and all you can do is to <ins>give Pac-man the instructions</ins> about the game only. On learning to find the best way to play the game with a maximum score, Pac-man has to <ins>play the game continuously, continuously, and continuously</ins>. That is the most basic understanding on what Reinforcement Learning looks like. <p align="center"><img width="50%" src="https://i.pinimg.com/originals/4a/f9/7b/4af97be15a1edae3f1b61cdb0a60d30a.gif " alt="illustration"></p></p><p>Mathematically, we could denote Pac-man as <strong>an agent</strong> and the game as <strong>the environment</strong>:</p><ul><li>One <ins>random strategy</ins> to play the Pac-man game as <strong>a policy</strong> (<code>œÄ ‚àà Œ†</code>).</li><li>One <ins>position of Pac-man at a specific time</ins> as <strong>a state</strong> (<code>s ‚àà S</code>).</li><li>An <ins>event that Pac-man moves</ins> forward (or backward of left or right) as <strong>an action</strong> (<b>a ‚àà A</b>).</li><li>A <ins>score that Pac-man received</ins> after taking an action as <strong>the reward</strong> (<code>r ‚àà R</code>). <p align="center"><img width="80%" src="https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png" alt="illustration"><div align="center"><b> Fig 1.</b> An illustration on Reinforcement Learning </div></p></li></ul><p>The ultimate goal of Reinforcement Learning is to find an optimal policy <b>œÄ<sup>*</sup></b> which gives the agent a maximum reward <b>r<sup><span>*</span></sup></b> on any diverse environments.</p><h2 id="_2-principal-mathematical-concepts-of-reinforcement-learning" tabindex="-1">2. Principal (mathematical) concepts of Reinforcement Learning <a class="header-anchor" href="#_2-principal-mathematical-concepts-of-reinforcement-learning" aria-hidden="true">#</a></h2><h3 id="a-transition-and-reward" tabindex="-1">a. Transition and Reward <a class="header-anchor" href="#a-transition-and-reward" aria-hidden="true">#</a></h3><p>Dummies&#39; reinforcement learning algorithms often require a model for the agent to learn and infer. A model is a descriptor in which our agent would contact with the environment and receive respective solution feedbacks. Two most contributing factors to receive accurate feedbacks on how the agent performs are <strong>transition probability function (P)</strong> and <strong>reward function (R)</strong>.</p><p>The <ins>transition probability function</ins> <strong>P</strong>, as it is called, records the probability of transitioning from state <strong>s</strong> to <strong>s‚Äô</strong> after taking action and get a reward <strong>r</strong> at time <strong>t</strong>.</p><p align="center"><b>P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) = <b>p</b>[<b>S</b><sub><b>t+1</b></sub> = <b>s&#39;</b>, <b>R</b><sub><b>t+1</b></sub> = <b>r</b> | <b>S</b><sub><b>t</b></sub> = <b>s</b>, <b>A</b><sub><b>t</b></sub> = <b>a</b>] </p><p>The equation could be understood as <em><strong>&quot;the transition probability of state s to s&#39; to get a reward r after taking action a equals to the conditional probability (p) of getting a reward of r after changing state from s to s&#39; and taking an action a&quot;</strong></em>.</p><p>And the <ins>transition state function</ins> could be defined as:</p><p align="center"><b>P<sub>ss&#39;, a</sub></b> = <b>P</b>(<b>s&#39; </b>|<b>s</b>,<b> a</b>) = <b><span>‚àë</span><sub>r ‚àà R</sub> P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) </p><p>The reward achieved on taking a new action <strong>a&#39;</strong> is demonstrated using <ins>a reward function</ins> <strong>R</strong> and equals to the expected reward after taking an action <strong>a</strong> in the state <strong>s</strong>.</p><p align="center"><b>R</b>(<b>s</b>, <b>a</b>) = <b>ùîº</b>[<b>R<sub>t + 1</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] = <b><span>‚àë</span><sub>r ‚àà R</sub></b>(<b>r</b>) x <b><span>‚àë</span><sub>s&#39; ‚àà S</sub>P</b>(<b>s&#39;, r</b> | <b>s, a</b>) </p><h3 id="b-policy" tabindex="-1">b. Policy <a class="header-anchor" href="#b-policy" aria-hidden="true">#</a></h3><p>A policy <strong>œÄ(<span>¬∑</span>)</strong> is a mapping from a state <strong>s</strong> to its corresponding action <strong>a</strong> according to a specific strategy to take a suitable move toward the current state of our agent. A policy could be deterministic or stochastic, as which demonstrated:</p><ul><li><b><ins>Stochastic policy</ins></b>: given a random state <strong>s</strong>, there is only one action <strong>a</strong> which always get the maximum reward <b>r<sup>*</sup></b>. <p align="center"><b>œÄ</b>(<b>s</b>) = <b>a</b></p></li><li><b><ins>Deterministic policy</ins></b>: given a random state <strong>s</strong>, the optimal action to get the highest reward is distributed over a probability distribution over all appropriate actions. <p align="center"><b>œÄ</b>(<b>a | s</b>) = <b>P</b><sub><b>œÄ</b></sub>[<b>A</b> = <b>a</b> | <b>S</b> = <b>s</b>] </p></li></ul><h3 id="c-state-value-and-action-value-function" tabindex="-1">c. State-Value and Action-Value Function <a class="header-anchor" href="#c-state-value-and-action-value-function" aria-hidden="true">#</a></h3><p>Value function, generally, is used as <ins>a metric to measure how good a state is and how high a reward we could achieve</ins> by staying in a state or taking an action.</p><p>Let denote <strong>G</strong> as the <b>total reward</b> that our agent would take, starting from time <strong>t</strong>: <p align="center"><b>G<sub>t</sub></b> = <b>R<sub>t+1</sub></b> + <b>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> = <b>0 ‚Üí</b><b><span>‚àû</span></b></sub><b>(R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><p>However, in reality, <ins>a reward would be more appetizing in the present than it is in the future</ins> (imagine which decision would you make if you are allowed to choose between receiving 100 bucks now or 100 bucks in 10 more years). It is, thus, a must to propose a coefficient of <strong>Œ≥ ‚àà [0; 1]</strong> to penalize rewards in the future times. The above equation would be rewritten as follows <p align="center"><b>G<sub>t</sub></b> = <b>Œ≥R<sub>t+1</sub></b> + <b>Œ≥<sup>2</sup>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> ‚àà [<b>0</b>;<b><span>‚àû</span></b>)</sub><b>(Œ≥<sup>k</sup> R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><h4 id="action-value-function" tabindex="-1">Action-Value function <a class="header-anchor" href="#action-value-function" aria-hidden="true">#</a></h4><p>The <ins>Action-Value value</ins> (Q-value) is the expected total reward return of a <strong>state-action pair</strong> at a given time <strong>t</strong> and following policy <b>œÄ</b>: <p align="center"><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] </p></p><h4 id="state-value-function" tabindex="-1">State-Value function <a class="header-anchor" href="#state-value-function" aria-hidden="true">#</a></h4><p>The <ins>State-Value</ins> value of a state <strong>s</strong> is the expected total reward return if we are in state <strong>s</strong> at time <strong>t</strong><p align="center"><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>] </p></p><p>On stochastic policy, <ins>the relationship between Action-Value and State-Value</ins> is demonstrated as: <div align="center"><p><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b><span>‚àë</span><sub>a ‚àà A</sub></b> [<b>œÄ</b>(<b>a</b> | <b>s</b>) * <b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p><p><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b><span>‚àë</span><sub>s&#39; ‚àà S</sub></b>[<b>P</b>(<b>s&#39;</b> | <b>s</b>, <b>a</b>) [<b>R</b>(<b>s</b>, <b>a</b>, <b>s&#39;</b>) + <b>Œ≥V<sub>œÄ</sub></b>(<b>s&#39;</b>)]] </p></div></p><h4 id="optimal-value-and-policy" tabindex="-1">Optimal value and policy <a class="header-anchor" href="#optimal-value-and-policy" aria-hidden="true">#</a></h4><p>By iterating around State-Value and Action-Value function, we could get the maximum return of the State-Value <p align="center"><b>V<sub><span>*</span></sub></b>(<b>s</b>) = <b>max<sub>œÄ</sub></b>{<b>V<sub>œÄ</sub></b>(<b>s</b>), <b>Q<sub><span>*</span></sub></b>(<b>s</b>, <b>a</b>)} = <b>max<sub>œÄ</sub></b>[<b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p></p><p>The optimal policy would be the argmax from the optimal policy from the aforementioned maximum return of the State-Value</p><p align="center"><b>œÄ<sub>*</sub></b> = argmax<sub><b>œÄ</b></sub>{<b>V<sub>œÄ</sub></b>(<b>s</b>) <b>œÄ<sub><span>*</span></sub></b>} </p><h2 id="_3-a-literature-on-preliminary-approaches-in-reinforcement-learning" tabindex="-1">3. A literature on preliminary approaches in Reinforcement Learning <a class="header-anchor" href="#_3-a-literature-on-preliminary-approaches-in-reinforcement-learning" aria-hidden="true">#</a></h2><p>In this section, I would drive you through the very initial solution for Reinforcement Learning</p><h3 id="a-dynamic-programming" tabindex="-1">a. Dynamic Programming <a class="header-anchor" href="#a-dynamic-programming" aria-hidden="true">#</a></h3><p>Dynamic Programming is the first thought coming into Computer Scientists when Reinforcement Learning had just appeared as a successive of previous mathematics foundations such as Bellman Equations [<a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_71" target="_blank" rel="noopener noreferrer">Bellman (1959)</a>] and Markov Decision Processes [<a href="https://dl.acm.org/doi/10.5555/528623" target="_blank" rel="noopener noreferrer">Puterman (1994)</a>]. There are a great number of DP-based and DP-inspired algorithms to deal with Reinforcement Learning. There lie two most highlighted algorithms when mentioned about DP in RL which are <b><ins>Policy Iteration</ins></b> and <b><ins>Value Iteration</ins></b>.</p><p>Both algorithms are DP algorithms based on Bellman Equations which aimed to find an optimal policy <b>œÄ<sub>*</sub></b> in a Reinforcement Learning environment using <ins>one-step look-ahead strategy</ins>.</p><table><thead><tr><th style="text-align:center;">Policy Iteration</th><th style="text-align:center;">Value Iteration</th></tr></thead><tbody><tr><td style="text-align:center;">Faster</td><td style="text-align:center;">Slower</td></tr><tr><td style="text-align:center;">Fewer iterations</td><td style="text-align:center;">More iterations</td></tr><tr><td style="text-align:center;">Guaranteed to converge</td><td style="text-align:center;">Guaranteed to converge</td></tr><tr><td style="text-align:center;">Algorithm is more <b>sophisticated</b></td><td style="text-align:center;">Algorithm is more <b>trouble-free</b></td></tr><tr><td style="text-align:center;">Start with <b>a random policy</b></td><td style="text-align:center;">Start with <b>a random value function</b></td></tr><tr><td style="text-align:center;"><ins>Cheaper</ins> computational costs</td><td style="text-align:center;"><ins>More expensive</ins> computational costs</td></tr></tbody></table><p align="center"><b>Table 1. </b>A minor comparison between Policy Iteration and Value Iteration. </p><h4 id="policy-iteration" tabindex="-1">Policy Iteration <a class="header-anchor" href="#policy-iteration" aria-hidden="true">#</a></h4><p>In policy iteration, one fixed policy is utilized firstly. Then, the algorithm would traverse through iterations, on every iteration, the policy is evaluated and updated gradually until the convergence.</p><div class="language-python"><pre><code><span class="token keyword">def</span> <span class="token function">policy_iteration</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> max_iters<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Intialize a random policy</span>
    policy <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>policy<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">import</span> random
        policy<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        policy_stable <span class="token operator">=</span> <span class="token boolean">True</span> 

        <span class="token comment"># Evaluate the present policy</span>
        v_values <span class="token operator">=</span>  policy_evaluation<span class="token punctuation">(</span>env<span class="token punctuation">,</span> 
                                      max_iters<span class="token operator">=</span>max_iters<span class="token punctuation">,</span> 
                                      gamma<span class="token operator">=</span>gamma<span class="token punctuation">,</span> 
                                      policy<span class="token operator">=</span>policy<span class="token punctuation">)</span>

        <span class="token comment"># Update the policy</span>
        new_policy <span class="token operator">=</span> policy_improvement<span class="token punctuation">(</span>env<span class="token punctuation">,</span> 
                                        gamma<span class="token operator">=</span>gamma<span class="token punctuation">,</span> 
                                        v_val<span class="token operator">=</span>v_values<span class="token punctuation">,</span> 
                                        policy<span class="token operator">=</span>policy<span class="token punctuation">)</span>

        <span class="token comment"># Check convergence</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>array_equiv<span class="token punctuation">(</span>policy<span class="token punctuation">,</span> new_policy<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
            policy_stable <span class="token operator">=</span> <span class="token boolean">True</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Converged at </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">-th iteration&#39;</span></span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            policy_stable <span class="token operator">=</span> <span class="token boolean">False</span>
        policy <span class="token operator">=</span> new_policy<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> policy
</code></pre></div><h4 id="value-iteration" tabindex="-1">Value Iteration <a class="header-anchor" href="#value-iteration" aria-hidden="true">#</a></h4><p>Value Iteration, on the other hand, begins with <ins>a selected value function</ins> and iterates over that value function. Value Iteration undergoes a heavier computing process by taking a maximum over the utility function for <ins>all possible actions</ins>. And subsequently, Value Iteration has a tendency to take more iterations before fully converged compared to the Policy one.</p><div class="language-python"><pre><code><span class="token keyword">def</span> <span class="token function">value_iteration</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> max_iters<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>
    v_values <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        prev_v_values <span class="token operator">=</span> np<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>v_values<span class="token punctuation">)</span>

        <span class="token comment"># Compute the value for state</span>
        <span class="token keyword">for</span> state <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            q_values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token comment"># Compute the q-value for each action</span>
            <span class="token keyword">for</span> action <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
                q_value <span class="token operator">=</span> <span class="token number">0</span>
                <span class="token comment"># Loop through each possible outcome</span>
                <span class="token keyword">for</span> prob<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token keyword">in</span> env<span class="token punctuation">.</span>P<span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">[</span>action<span class="token punctuation">]</span><span class="token punctuation">:</span>
                    q_value <span class="token operator">+=</span> prob <span class="token operator">*</span> <span class="token punctuation">(</span>reward <span class="token operator">+</span> gamma <span class="token operator">*</span> prev_v_values<span class="token punctuation">[</span>next_state<span class="token punctuation">]</span><span class="token punctuation">)</span>
                
                q_values<span class="token punctuation">.</span>append<span class="token punctuation">(</span>q_value<span class="token punctuation">)</span>
            
            <span class="token comment"># Select the best action</span>
            best_action <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span>
            v_values<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> q_values<span class="token punctuation">[</span>best_action<span class="token punctuation">]</span>
        
        <span class="token comment"># Check convergence</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>isclose<span class="token punctuation">(</span>v_values<span class="token punctuation">,</span> prev_v_values<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Converged at </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">-th iteration.&#39;</span></span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>
    
    <span class="token keyword">return</span> v_values
</code></pre></div><h3 id="b-q-learning" tabindex="-1">b. Q-Learning <a class="header-anchor" href="#b-q-learning" aria-hidden="true">#</a></h3><p>Q-Learning is <ins>a <b>model-free</b> reinforcement learning algorithm</ins> to learn the quality of action <strong>a</strong> to tell the agent which actions to execute at a certain state <strong>s</strong>. It does not require a model of a certain environment. It can handle dynamically environmental problems and achieve rewards without any adaptations. The development of Q-Learning is actually a breakthrough in the early days of reinforcement learning.</p><p>The interesting point in Q-Learning is the independence of the current policy to choose the second action <b>a<sub>t+1</sub></b>. It, essentially, estimates <b>Q<sub><span>*</span></sub></b> among the best Q-values, but it does not matter which action leads to this maximum Q-value and in the next step, this algorithm does not have to follow that policy.</p><div align="center"><img width="80%" src="/q-learning.png" alt="q-learning"><div align="center"> Pseudo code for Updating Q-Value <a href="https://github.com/DTA-UIT/Deep-Q-Network/blob/main/report/Deep%20Q%20Network.pdf">[Tan Ngoc Pham et al. (2020)]</a></div></div><h2 id="_4-conclusions" tabindex="-1">4. Conclusions <a class="header-anchor" href="#_4-conclusions" aria-hidden="true">#</a></h2><ul><li>Throughout this blog, I have brought to you <ins>the most preliminary concepts</ins> on what Reinforcement Learning looks like, its mathematical fundamental and some of the algorithms to deal with Reinforcement Learning problems.</li><li>Further construction in Reinforcement Learning and more intensive algorithms surrounding would be settled on later posts on this blog.</li></ul></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neural Architecture Search]]></title>
            <link>https://blog.vuejs.org/posts/neural-architecture-search.html</link>
            <guid>https://blog.vuejs.org/posts/neural-architecture-search.html</guid>
            <pubDate>Sat, 12 Feb 2022 12:00:00 GMT</pubDate>
            <description><![CDATA[ 
The needs for appropriate deep neural networks for most modern problems have increased dramatically, and finding the right architecture is a challenge. Almost deep architectures nowadays are manually designed with tremendous times of trial-and-errors and state-of-the-arts are those with multiple layers,  connections ignoring a suitable explanation. 

]]></description>
            <content:encoded><![CDATA[<div><p>The needs for appropriate deep neural networks for most modern problems have increased dramatically, and finding the right architecture is a challenge. Almost deep architectures nowadays are manually designed with tremendous times of trial-and-errors and state-of-the-arts are those with multiple layers, connections ignoring a suitable explanation.</p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ul><li><ol><li><a href="#:~:text=1.%20Introduction">Introduction</a></li></ol></li><li><ol start="2"><li><a href="#:~:text=2.%20Recent%20literature%20on%20Neural%20Architecture%20Search">Recent literature on Neural Architecture Search</a></li></ol></li><li><ol start="3"><li><a href="#:~:text=3.%20Conclusions">Conclusion</a></li></ol></li></ul><hr><h3 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h3><ul><li>NAS&#39;s concept begins with a <em><strong>search space</strong></em> of S, which could be under the form of different to-find-layers (macro-level), various operations (micro-level) or even different complete architectures that we would like to find. The paradigm then flows to the <em><strong>search strategy</strong></em> in which our heuristics would perform their strength towards the problem, e.g. Genetic Algorithm, Reinforcement Learning.</li><li>On applying the suitable search strategy to the search space, one (or many) architectures is picked up from the search space and put into the <em><strong>evaluation step</strong></em>. The chosen architecture is evaluated by real training on the given dataset and the fitness is returned as the form of our wanted metric for the problem.</li><li>This whole procedure is <em><strong>repeated continuously</strong></em> until we found out the outperformed architecture for the problem or the algorithm met the <em><strong>terminated criterion</strong></em>, i.e. number of iterations, out of computational resources. <p align="center"><img width="80%" src="/NAS.png" alt="paradigm"><div align="center"><b> Fig 1.</b> Paradigm of NAS </div></p></li></ul><h3 id="_2-recent-literature-on-neural-architecture-search" tabindex="-1">2. Recent literature on Neural Architecture Search <a class="header-anchor" href="#_2-recent-literature-on-neural-architecture-search" aria-hidden="true">#</a></h3><h4 id="a-neural-optimizer-search-with-reinforcement-learning" tabindex="-1">a. <a href="http://proceedings.mlr.press/v70/bello17a/bello17a.pdf" target="_blank" rel="noopener noreferrer">Neural Optimizer Search with Reinforcement Learning</a> <a class="header-anchor" href="#a-neural-optimizer-search-with-reinforcement-learning" aria-hidden="true">#</a></h4><p>Mentioning NAS often means mentioning the work of <a href="http://proceedings.mlr.press/v70/bello17a/bello17a.pdf" target="_blank" rel="noopener noreferrer">Zoph and Le. 2017</a> on first bringing out the concept of finding neural architecture and how to resolve the problem. The very first solution for the NAS is to use a <em><strong>Recurrent Neural Network controller</strong></em> to generate a string representating the outcome network using <strong>Reinforcement Learning</strong> in which:</p><ul><li>The <ins>environment</ins> for the NAS is the search space of the problem.</li><li>The <ins>actions</ins> are the search strategies that we would love to apply on.</li><li>The <ins>reward</ins> is the accuracy of the chosen architecture trained on CIFAR-10 dataset.</li></ul><p>The results towards the initial approach is outstanding when the final architectures have a tendency to <strong>converge near the state-of-the-art architectures</strong> that we know nowadays, e.g. <strong>AlexNet</strong> [<a href="https://dl.acm.org/doi/10.5555/2999134.2999257" target="_blank" rel="noopener noreferrer">Krizhevsky et al. (2012)</a>] or <strong>VGG</strong> [<a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener noreferrer">Simonyan and Zisserman (2014)</a>]. And moreover, the result architecture is not overfitted on the train dataset but has a strong transferability when testing on different datasets.</p><p>However, the approach costs a fortune of computational resources since each step, they have to construct the network from the beginning and the network has to be trained from scratch. The power of the very first method of NAS and the limitations from computational process is the primary motivation for later works on various aspects which are search spaces, datasets, training-free methods and so on.</p><h4 id="b-nas-benchmark" tabindex="-1">b. NAS Benchmark: <a class="header-anchor" href="#b-nas-benchmark" aria-hidden="true">#</a></h4><p>On resolving the problem of training from scratch on the way, NAS Benchmark has been born to minimize the computational costs that researchers demand when doing the NAS. There are a great amount of NAS Benchmarks nowadays but there are 2 most highlighted ones in the field.</p><ul><li><strong>NAS-Bench-101 [<a href="https://arxiv.org/abs/1902.09635" target="_blank" rel="noopener noreferrer">Ying et al. (2019)</a>]</strong><br> Developed by Google, NAS-Bench-101 is constructed with a compact, yet expressive, search space, exploiting graph isomorphisms to identify <strong>423 000 unique convolutional architectures</strong> on CIFAR-10 dataset. NAS-Bench-101 is simply used by querying based on the upper-triangular matrix representating the formulation of the network we would like to evaluate.</li></ul><div class="language-python"><pre><code><span class="token comment"># Load the data from file (this will take some time)</span>
nasbench <span class="token operator">=</span> api<span class="token punctuation">.</span>NASBench<span class="token punctuation">(</span><span class="token string">&#39;/path/to/nasbench.tfrecord&#39;</span><span class="token punctuation">)</span>

<span class="token comment"># Create an Inception-like module (5x5 convolution replaced with two 3x3</span>
<span class="token comment"># convolutions).</span>
model_spec <span class="token operator">=</span> api<span class="token punctuation">.</span>ModelSpec<span class="token punctuation">(</span>
    <span class="token comment"># Adjacency matrix of the module</span>
    matrix<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># input layer</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 1x1 conv</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 3x3 conv</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 5x5 conv (replaced by two 3x3&#39;s)</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 5x5 conv (replaced by two 3x3&#39;s)</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment"># 3x3 max-pool</span>
            <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>   <span class="token comment"># output layer</span>
    <span class="token comment"># Operations at the vertices of the module, matches order of matrix</span>
    ops<span class="token operator">=</span><span class="token punctuation">[</span>INPUT<span class="token punctuation">,</span> CONV1X1<span class="token punctuation">,</span> CONV3X3<span class="token punctuation">,</span> CONV3X3<span class="token punctuation">,</span> CONV3X3<span class="token punctuation">,</span> MAXPOOL3X3<span class="token punctuation">,</span> OUTPUT<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># Query this model from dataset, returns a dictionary containing the metrics</span>
<span class="token comment"># associated with this model.</span>
data <span class="token operator">=</span> nasbench<span class="token punctuation">.</span>query<span class="token punctuation">(</span>model_spec<span class="token punctuation">)</span>
</code></pre></div><p>The result achieved on this benchmark is under the format of a json:</p><div class="language-python"><pre><code>data <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&#39;module_adjacency&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;module_operations&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;trainable_parameters&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;training_time&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;train_accuracy&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;validation_accuracy&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;test_accuracy&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span>
<span class="token punctuation">}</span>
</code></pre></div><ul><li><strong>NATS-Bench [<a href="https://arxiv.org/abs/2009.00437" target="_blank" rel="noopener noreferrer">Dong et al. (2021)</a>]</strong><br> The NATS-Bench is the upgraded version of a previous work NAS-Bench-201. NATS-Bench includes the search space of <strong>15,625 neural cell candidates</strong> for architecture topology and <strong>32,768 for architecture size</strong> on three dataset (<strong>CIFAR-10</strong>, <strong>CIFAR-100</strong>, <strong>ImageNet16-120</strong>).</li></ul><div class="language-python"><pre><code><span class="token triple-quoted-string string">&quot;&quot;&quot;
Create the benchmark instance
&quot;&quot;&quot;</span>
<span class="token keyword">from</span> nats_bench <span class="token keyword">import</span> create
<span class="token comment"># Create the API instance for the size search space in NATS</span>
api <span class="token operator">=</span> create<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">&#39;sss&#39;</span><span class="token punctuation">,</span> fast_mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># Create the API instance for the topology search space in NATS</span>
api <span class="token operator">=</span> create<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token string">&#39;tss&#39;</span><span class="token punctuation">,</span> fast_mode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&quot;&quot;&quot;
Query the performance
&quot;&quot;&quot;</span>
<span class="token comment"># Query the loss / accuracy / time for 1234-th candidate architecture on CIFAR-10</span>
<span class="token comment"># info is a dict, where you can easily figure out the meaning by key</span>
info <span class="token operator">=</span> api<span class="token punctuation">.</span>get_more_info<span class="token punctuation">(</span><span class="token number">1234</span><span class="token punctuation">,</span> <span class="token string">&#39;cifar10&#39;</span><span class="token punctuation">)</span>

<span class="token comment"># Query the flops, params, latency. info is a dict.</span>
info <span class="token operator">=</span> api<span class="token punctuation">.</span>get_cost_info<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token string">&#39;cifar10&#39;</span><span class="token punctuation">)</span>
</code></pre></div><p>The result from querying NATS-Bench is a json of multiple metrics on 2 different epochs:</p><div class="language-python"><pre><code>info <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&#39;train-loss&#39;</span>     <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;train-accuracy&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;train-per-time&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;train-all-time&#39;</span> <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>  
    <span class="token string">&#39;test-loss&#39;</span>      <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;test-accuracy&#39;</span>  <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;test-per-time&#39;</span>  <span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> 
    <span class="token string">&#39;test-all-time&#39;</span>  <span class="token punctuation">:</span> <span class="token number">0</span>
<span class="token punctuation">}</span>
</code></pre></div><h4 id="_3-conclusions" tabindex="-1">3. Conclusions <a class="header-anchor" href="#_3-conclusions" aria-hidden="true">#</a></h4><ul><li>Manual construction on deep neural networks is a time-consumed, high computational consuming and specified works.</li><li>Neural Architecture Search is the field when we minimize the labour on evaluating separated architectures for a typical dataset.</li></ul></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Leukocyte classification to predict diseases (Part 2)]]></title>
            <link>https://blog.vuejs.org/posts/computer-vision-for-leukocyte-prediction-2.html</link>
            <guid>https://blog.vuejs.org/posts/computer-vision-for-leukocyte-prediction-2.html</guid>
            <pubDate>Thu, 23 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Data analysis on number of blood cells, which are white blood cells and red blood cells, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.

]]></description>
            <content:encoded><![CDATA[<div><p>Data analysis on number of blood cells, which are white blood cells and red blood cells, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.</p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ul><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html" target="_blank" rel="noopener noreferrer">Part 1</a><ul><li><ol><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=1.%20Introduction" target="_blank" rel="noopener noreferrer">Introduction</a></li></ol></li><li><ol start="2"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=2.%20Dataset%20and%20data%20preprocessing" target="_blank" rel="noopener noreferrer">Dataset and data preprocessing</a></li></ol></li><li><ol start="3"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=3.%20Foreground%20and%20background%20segmentation%20technique" target="_blank" rel="noopener noreferrer">Foreground and background segmentation technique</a></li></ol></li></ul></li><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html" target="_blank" rel="noopener noreferrer">Part 2</a><ul><li><ol start="4"><li><a href="#:~:text=4.%20Edge%20detection%20with%20Canny%20method">Edge detection with Canny method</a></li></ol></li><li><ol start="5"><li><a href="#:~:text=5.%20Hough%20transformation%20to%20identify%20blood%20cells%20borderlines">Hough transformation to identify blood cells borderlines</a></li></ol></li><li><ol start="6"><li><a href="#:~:text=6.%20References">References</a></li></ol></li></ul></li></ul><hr><h4 id="_4-edge-detection-with-canny-method" tabindex="-1">4. Edge detection with Canny method <a class="header-anchor" href="#_4-edge-detection-with-canny-method" aria-hidden="true">#</a></h4><p>This method uses both high and low as 2 separated thresholds. The high one would be used firstly to find the starting point of the edge. After that, we determine the path direction of the border based on consecutive pixels having a greater value in comparison of the low. Points with the values less than the low threshold are removed only. Tiny borders will be selected if they are associated with large (easy to see) borders. The Canny procedure is demonstrated as follows:</p><ul><li><p><strong>Step 1</strong>: Use a Gaussian filter to smoothen the image. This step is meant to reduce the gradient of pixels while moving from one pixel to another and make the image smoother than the original.</p></li><li><p><strong>Step 2</strong>: Calculate the gradient of the contour of the smoothed image. Computing the gradient is mainly to construct a function to redefine the slope as well as the increasing and decreasing trend of a certain pixel.</p></li><li><p><strong>Step 3</strong>: Remove the non-maxima points. As a side note, we will choose the pixels that are most likely to be considered the highest edge by relying on the gradient calculation in step 2. When a pixel has not reached its peak, we won&#39;t evaluate that pixel as an edge.</p></li><li><p><strong>Step 4</strong>: The final step is to remove the values that are less than the threshold level. Should you observe an image carefully, there are many values after calculating the gradient that will be selected as the maximum value. However, the maximum value here is only local maximum and only a few points are considered as global maximum. Therefore, we will remove some regions in which the local maximum value is lower than the allowable threshold (the threshold value will depend on the type of model to choose more appropriate).</p></li></ul><p>This method is considered as more superior to other methods because it is less affected by noises and it is also able to detect weak edges. On the other hand, if the threshold is chosen too low, the boundary will be incorrect, or if the threshold is chosen too high, much of the important information about the boundary will be discarded. Based on the predefined threshold level, it will decide which points belong to the true boundary or not to the boundary. The lower the threshold level, the more edges are detected (but also noise and false edges appear). Conversely, if the threshold level is set higher, the fuzzy borders may be lost, or the borders will be broken.</p><h4 id="_5-hough-transformation-to-identify-blood-cells-borderlines" tabindex="-1">5. Hough transformation to identify blood cells borderlines <a class="header-anchor" href="#_5-hough-transformation-to-identify-blood-cells-borderlines" aria-hidden="true">#</a></h4><p>Hough transformation is one of the most basic methods for feature extraction in image processing, and especially one of its capabilities is to find and detect circles in images (even imperfect circles).</p><p>This method mainly relies on the parameter accumulator after being added up (called ‚Äúvoting‚Äù) to get the maximum values. We will identify objects that are supposed to be circular in image with the following formula:</p><p align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;r^2&amp;space;=&amp;space;(x&amp;space;-&amp;space;a)^2&amp;space;+&amp;space;(y&amp;space;-&amp;space;b)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{150}&amp;space;r^2&amp;space;=&amp;space;(x&amp;space;-&amp;space;a)^2&amp;space;+&amp;space;(y&amp;space;-&amp;space;b)^2" title="r^2 = (x - a)^2 + (y - b)^2"></a></p><p>s.t. <strong>r</strong> is radius of circle, <code>(a, b)</code> is the circle&#39;s center.</p><p>The <strong>x</strong>, <strong>y</strong> values only move in the range <code>|r - a| ‚â§ x ‚â§ |r + a|</code> and |<code>r - b| ‚â§ y ‚â§ |r + b|</code> so that the above formula is satisfied. The implementation of the algorithm to &#39;voting&#39; the positions will be like the setting in determining the circle as above. Here we perform &#39;voting&#39; on a 3D matrix with parameters <strong>r</strong>, <strong>a</strong>, <strong>b</strong>.</p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><p align="center"><img width="80%" src="/CircleHoughTransform1.png" alt="Simulate two circles with difference sizes"><div align="center"><figcaption><b>Fig 5.</b> Simulate two circles </figcaption><figcaption>with difference sizes</figcaption></div></p></div><div><p align="center"><img width="80%" src="/CircleHoughTransform2.png" alt="Cumulative table of 2 parameters x, y with fixed r"><div align="center"><figcaption><b>Fig 6.</b> Cumulative table of 2 parameters x, y </figcaption><figcaption> with fixed r</figcaption></div></p></div></div><div align="center"><b>Fig 5,6.</b> Simulating the process of determining a circle using the Hough transform </div><p>For ease of visualization, we draw two small circles. Our task is to help the computer determine the center of two circles. Pay attention to the circle highlighted in blue and you will see that they have a smaller radius than the radius of the green circle. We have estimated the radius distance of the blue circle to be <code>r = 20 pixels</code>.</p><p>So, for each pixel that is said to be the edge (with the image on the edge being colored both blue or green), then proceed to draw a new circle with the center at the position in the cell under consideration - call <code>x = position_row position</code> and <code>y = column_position</code>.</p><p>After obtaining the three values of <strong>x</strong>, <strong>y</strong> and <strong>r</strong>, the two values <strong>a</strong> and <strong>b</strong> can be deduced easily as follows:</p><div class="language-python"><pre><code>PI <span class="token operator">=</span> <span class="token number">3.14159265358979323846264338327950</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">361</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    b <span class="token operator">=</span> y <span class="token operator">-</span> r <span class="token operator">*</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>t<span class="token operator">*</span>PI<span class="token operator">/</span><span class="token number">180</span><span class="token punctuation">)</span> 
    a <span class="token operator">=</span> x <span class="token operator">-</span> y <span class="token operator">*</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>t<span class="token operator">*</span>PI<span class="token operator">/</span><span class="token number">180</span><span class="token punctuation">)</span> 
    vote<span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span> 
</code></pre></div><p>As above, we will select the threshold value and take the positions vote<code>[a, b] &gt; threshold</code>. The higher the threshold value is, the more likely it is to be a perfect circle. However, when the threshold value is too low, we are not sure that it is a circle (maybe even a small curve). In practice, it is virtually impossible to determine the radius in a fixed way, even if it is a human or a program with the best perception. So, when we practice in practice, we choose <strong>r</strong> in some interval <code>[m, n]</code>. This means that the voting array will increase from <strong>2D</strong> to <strong>3D</strong> as <code>[a, b, r]</code>.</p><h4 id="_6-references" tabindex="-1">6. References <a class="header-anchor" href="#_6-references" aria-hidden="true">#</a></h4><p>[1] L. Chandrasekar and G. Durga. Implementation of hough transform for image processing applications. In 2014 International Conference on Communication and Signal Processing, pages 843‚Äì847, 2014. <em>doi: 10.1109/ICCSP.2014.6949962</em>.</p><p>[2] J. Illingworth and J. Kittler. The adaptive hough transform. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-9(5):690‚Äì698, 1987. <em>doi: 10.1109/TPAMI.1987.4767964</em>.</p><p>[3] H. Kang, D.-Y. Kang, J.-S. Park, and S. W. Ha. Vgg19-based classification of amyloid pet image in patients with mci and ad. In 2018 International Conference on Computational Science and Computational Intelligence (CSCI), pages 1442‚Äì1443, 2018. <em>doi: 10.1109/CSCI46756.2018.00281</em>.</p><p>[4] T. Kaur and T. K. Gandhi. Automated brain image classification based on vgg-16 and transfer learning. In 2019 International Conference on Information Technology (ICIT), pages 94‚Äì98, 2019. <em>doi: 10.1109/ICIT48102.2019.00023</em>.</p><p>[5] S. Liu and W. Deng. Very deep convolutional neural network based image classification using small training sample size. In 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), pages 730‚Äì734, 2015. <em>doi: 10.1109/ACPR.2015.7486599</em>.</p><p>[6] Raghu, N. Sriraam, Y. Temel, S. V. Rao, and P. L. Kubben. A convolutional neural network based framework for classification of seizure types. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 2547‚Äì2550, 2019. <em>doi: 10.1109/EMBC.2019.8857359</em>.</p><p>[7] M. Raman and H. Aggarwal. Study and comparison of various image edge detection techniques. <em>International Journal of Image Processing, 3, 03 2009</em>.</p><p>[8] M. Shaha and M. Pawar. Transfer learning for image classification. In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), pages 656‚Äì660, 2018. <em>doi: 10.1109/ICECA.2018.8474802</em>.</p><p>[9] S. Singh and R. Singh. Comparison of various edge detection techniques. In 2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom), pages 393‚Äì396, 2015.</p><p>[10] L. Wen, X. Li, X. Li, and L. Gao. A new transfer learning based on vgg-19 network for fault diagnosis. In 2019 IEEE 23rd International Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 205‚Äì209, 2019. <em>doi: 10.1109/CSCWD.2019.8791884</em>.</p><p>[11] H. Ye, G. Shang, L. Wang, and M. Zheng. A new method based on hough transform for quick line and circle detection. In 2015 8th International Conference on Biomedical Engineering and Informatics (BMEI), pages 52‚Äì56, 2015. <em>doi: 10.1109/BMEI.2015.7401472</em>.</p><p>[12] R. M. Yousaf, H. A. Habib, H. Dawood, and S. Shafiq. A comparative study of various edge detection methods. In 2018 14th International Conference on Computational Intelligence and Security (CIS), pages 96‚Äì99, 2018. <em>doi: 10.1109/CIS2018.2018.00029</em>.</p><p>[13] Rezatofighi, S.H., Soltanian-Zadeh, H.: Automatic recognition of five types of white blood cells in peripheral blood. Computerized Medical Imaging and Graphics 35(4) (2011) 333--343.</p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Leukocyte classification to predict diseases (Part 1)]]></title>
            <link>https://blog.vuejs.org/posts/computer-vision-for-leukocyte-prediction.html</link>
            <guid>https://blog.vuejs.org/posts/computer-vision-for-leukocyte-prediction.html</guid>
            <pubDate>Wed, 22 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Data analysis on number of blood cells, which are white blood cells and red blood cells, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.

]]></description>
            <content:encoded><![CDATA[<div><p>Data analysis on number of blood cells, which are white blood cells and red blood cells, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.</p><hr><p>Since this blog is rather long, we divide it into 2 parts, the first one is to introduce our approach to the problem, our dataset and our approach to preprocess data, then the second one is to discuss further about how to identify our needed cells in the preprocessed data through Canny object detection and Hough transformation.</p><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ul><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html" target="_blank" rel="noopener noreferrer">Part 1</a><ul><li><ol><li><a href="#:~:text=1.%20Introduction">Introduction</a></li></ol></li><li><ol start="2"><li><a href="#:~:text=2.%20Dataset%20and%20data%20preprocessing">Dataset and data preprocessing</a></li></ol></li><li><ol start="3"><li><a href="#:~:text=3.%20Foreground%20and%20background%20segmentation%20technique">Foreground and background segmentation technique</a></li></ol></li></ul></li><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html" target="_blank" rel="noopener noreferrer">Part 2</a><ul><li><ol start="4"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html#:~:text=4.%20Edge%20detection%20with%20Canny%20method" target="_blank" rel="noopener noreferrer">Edge detection with Canny method</a></li></ol></li><li><ol start="5"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html#:~:text=5.%20Hough%20transformation%20to%20identify%20blood%20cells%20borderlines" target="_blank" rel="noopener noreferrer">Hough transformation to identify blood cells borderlines</a></li></ol></li><li><ol start="6"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html#:~:text=6.%20References" target="_blank" rel="noopener noreferrer">References</a></li></ol></li></ul></li></ul><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><p>While blood cells (WBC), also known as <em>leukocytes</em>, are produced in the bone marrow and composed of nuclei and cytoplasm. WBCs are divided into five groups: basophil, eosinophil, lymphocyte, monocyte and neutrophil. Leukocytes protects the body against infectious disease and foreign substance, constitute an important part of the immune system. A healthy adult human consists of 4.1e9 to 11.1e9 of WBCs per a blood liter (or 4,500 to 11,000 WBCs per a blood microliter) and a drop in a blood consists of 7,000 to 25,000 WBCs. Any statistical number that outranges taken from an adult is considered as a disease.</p><div align="center"><table><thead><tr><th><p align="center"> Name</p></th><th><p align="center"> Description </p></th></tr></thead><tbody><tr><td><p align="center"> Neutrophils </p></td><td><p align="center"> Contact the microbial invasion, phagocytize and destroy invading organism </p></td></tr><tr><td><p align="center"> Eosinophils </p></td><td><p align="center"> Part of defense mechanism </p><p align="center">against parasitic infections, inflammatory processes and allergic tissue reactions </p></td></tr><tr><td><p align="center"> Basophils </p></td><td><p align="center"> Play role in allergic and immediate hypersensitivity reactions </p></td></tr><tr><td><p align="center"> Monocytes </p></td><td><p align="center"> Involve in defensive reactions to some microorganisms, remove damaged cells, cell debris</p><p align="center"> and armour bactericidal action as immune reaction </p></td></tr><tr><td><p align="center"> Lymphocytes </p></td><td><p align="center"> Produce antibodies and join in immune reactions </p></td></tr></tbody></table><p><b>Table 1. </b>Names and brief demonstration of normal leukocytes.</p></div><h4 id="_2-dataset-and-data-preprocessing" tabindex="-1">2. Dataset and data preprocessing <a class="header-anchor" href="#_2-dataset-and-data-preprocessing" aria-hidden="true">#</a></h4><p>The <a href="http://users.cecs.anu.edu.au/~hrezatofighi/Data/Leukocyte%20Data.htm" target="_blank" rel="noopener noreferrer">LISC - <em>Leukocyte Images for Segmentation and Classification</em></a> is used for automatic identification and blood cells counting since it is relatively easy to use.</p><blockquote><p>Samples were taken from peripheral blood of 8 normal subjects and 400 samples were obtained from 100 microscope slides. The microscope slides were smeared and stained by Gismo-Right technique and images were acquired by a light microscope (Microscope-Axioskope 40) from the stained peripheral blood using an achromatic lens with a magnification of 100. Then, these images were recorded by a digital camera and were saved in the BMP format. The images contain 720√ó576 pixels.</p></blockquote><blockquote><p>All of them are color images. The images were classified by a hematologist into normal leukocytes: basophil, eosinophil, lymphocyte, monocyte, and neutrophil. Also, the areas related to the nucleus and cytoplasm were manually segmented by an expert.</p></blockquote><p>You can download dataset here: <a href="http://users.cecs.anu.edu.au/~hrezatofighi/Data/Leukocyte%20Data.htm" target="_blank" rel="noopener noreferrer">LISC database</a>.</p><p>After that, images are transformed into gray scale using Python&#39;s <a href="https://pypi.org/project/opencv-python/" target="_blank" rel="noopener noreferrer">opencv</a> library:</p><div class="language-python"><pre><code><span class="token keyword">import</span> cv2
gray_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>original_img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2GRAY<span class="token punctuation">)</span>
</code></pre></div><p>And after using opencv, we can change a normal leukocyte into a grayscale leukocyte.</p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><img width="70%" src="https://github.com/BTrDung/Complex/raw/master/CreProjCBC/4.bmp" alt="leukocyte-before"></div><div><img width="40%" src="/grayscale-leukocyte.png" alt="leukocyte-after"></div></div><div align="center"><b>Fig 1.</b> Image of a normal leukocyte before and after using opencv </div><h4 id="_3-foreground-and-background-segmentation-technique" tabindex="-1">3. Foreground and background segmentation technique <a class="header-anchor" href="#_3-foreground-and-background-segmentation-technique" aria-hidden="true">#</a></h4><p>At the very first glance to the solution, we have to notice how to differentiate between foreground and background.</p><p>We could define foreground as an object that we want to observe detailedly. For a thorough understanding, a foregrounding object is the object we would like to separate from the whole picture for later purposes, e.g. object detecting or motion predicting. In this topic area, leukocytes and erythrocytes are defined as the foreground and moreover, we will not use background. Hence, at the end this section, we would guide you thoroughly to extract the foreground from the a whole picture.</p><p>Let make an overview about the image mentioned below:</p><p align="center"><img src="https://webpath.med.utah.edu/jpeg5/HEME005.jpg" alt="basophil"><div align="center"><figcaption><b>Fig 2.</b> Example of one blood cell photo from dataset </figcaption></div></p><p>Scientific evidence has it that dark purple areas are leukocytes and light purple areas are erythrocytes. We would divide the whole picture into 2 different ones: the first one only consists of leukocytes&#39; positions and the other consists of those from erythrocytes; and both of them would remain the original scale. The reason behind doing splitting thing is to make processing and classifying procedure more easily.</p><p>One of the most basic approach for this method is to use thresholding technique. The threshold formula could be expressed as below:</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;f(x,y)&amp;space;=&amp;space;\left\{\begin{matrix}&amp;space;255&amp;space;&amp;f(x,y)&amp;space;&gt;&amp;space;thres&amp;space;\\&amp;space;0&amp;space;&amp;f(x,y)\leq&amp;space;thres&amp;space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{150}&amp;space;f(x,y)&amp;space;=&amp;space;\left\{\begin{matrix}&amp;space;255&amp;space;&amp;f(x,y)&amp;space;&gt;&amp;space;\theta&amp;space;\\&amp;space;0&amp;space;&amp;f(x,y)\leq&amp;space;\theta&amp;space;\end{matrix}\right." title="f(x,y) = \left\{\begin{matrix} 255 &amp;f(x,y) &gt; \theta \\ 0 &amp;f(x,y)\leq thres \end{matrix}\right."></a> s.t. <em><strong>Œ∏</strong></em> is defined as our threshold</p><p>Since our image has been already transformed from RGB to gray-scale, all pixels are now in the threshold of from 0 to 255. We could get rid of background part by using <a href="https://www.investopedia.com/terms/h/histogram.asp" target="_blank" rel="noopener noreferrer"><em>histogram</em></a>, which is the graph for representation of the distribution of colors in an image, to analyse and choose an appropriate threshold afterwards (i recommend you could take the chosen threshold from the number with the highest wave frequency).</p><p align="center"><img width="80%" src="/histogram.png" alt="histogram"><div align="center"><figcaption><b>Fig 3.</b> Histogram of Figure 1 (threshold = 255). </figcaption></div></p><p>After that, the area with pixels valuing in <code>f(x,y) &gt; threshold</code> are categorized as the foreground and vice versa is the background to identify the erythrocytes. In addition, we also apply another formula to extract leukocytes from the total image, which is:</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&amp;space;\dpi{150}&amp;space;f(x,&amp;space;y)=\left\{\begin{matrix}&amp;space;255&amp;space;&amp;threshold_{min}&amp;space;\leq&amp;space;f(x,y)&amp;space;\leq&amp;space;threshold_{max}\\&amp;space;0&amp;space;&amp;f(x,&amp;space;y)&amp;space;\notin&amp;space;\[threshold_{min},threshold_{max}\]&amp;space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&amp;space;\dpi{150}&amp;space;f(x,&amp;space;y)=\left\{\begin{matrix}&amp;space;255&amp;space;&amp;\theta_{min}&amp;space;\leq&amp;space;f(x,y)&amp;space;\leq&amp;space;\theta_{max}\\&amp;space;0&amp;space;&amp;f(x,&amp;space;y)&amp;space;\notin&amp;space;\[\theta_{min},\theta_{max}\]&amp;space;\end{matrix}\right." title="f(x, y)=\left\{\begin{matrix} 255 &amp;threshold_{min} \leq f(x,y) \leq threshold_{max}\\ 0 &amp;f(x, y) \notin \[threshold_{min},threshold_{max}\] \end{matrix}\right."></a></p><p>After using our proposed method, we would achieve the result as followed: <div align="center"><img width="50%" src="/grayscale-leukocyte.png" alt="grayscale leukocyte"><figcaption><b>Fig 4.</b> A grayscale cellular image to be processed</figcaption></div></p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><p align="center"><img width="80%" src="/WBCs.png" alt="White blood cells image after using threshold"><div align="center"><figcaption><b>Fig 5.</b> Leukocytes after categorized as foreground </figcaption></div></p></div><div><p align="center"><img width="80%" src="/RBC.png" alt="Red blood cells image after using threshold"><div align="center"><figcaption><b>Fig 6.</b> Erythrocytes after categorized as foreground </figcaption></div></p></div></div><p>Our next blog would discuss further about how to detect and count blood cells in preprocessed images.</p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Real-time emotion recognizing]]></title>
            <link>https://blog.vuejs.org/posts/emotion-recognizing.html</link>
            <guid>https://blog.vuejs.org/posts/emotion-recognizing.html</guid>
            <pubDate>Mon, 20 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Introduce a deep learning approach to the problem of recognizing emotions in real time. 
<p align="center">
  <img width=400px height=100% src="/demo-emotion-recognizing.png" alt="demo-emotion">
</p>

]]></description>
            <content:encoded><![CDATA[<div><p>Introduce a deep learning approach to the problem of recognizing emotions in real time. <p align="center"><img width="400px" height="100%" src="/demo-emotion-recognizing.png" alt="demo-emotion"></p></p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ol><li><a href="#:~:text=1.%20Introduction">Introduction</a></li><li><a href="#:~:text=2.%20Dataset">Dataset</a></li><li><a href="#:~:text=3.%20Proposed%20architecture">Proposed architecture</a></li><li><a href="#:~:text=4.%20References">References</a></li></ol><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><p>There is a large number of neural networks nowadays to help us in almost every aspect of our life. In addition, we realize that different problems often require different types of networks. In this problem, I choose to use VGGFace network, or it is also called as Deep Face architecture.</p><p>VGGFace architecture was first introduced to solve the problem of recognizing humans&#39; face (i think it is called as Deep Face at first due to that reason, in my opinion).</p><div align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/IQ9qnqSi3gc" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><p>With that reason in mind, i think that VGGFace would perform well on other problems relevant to our faces as in the problem of emotion recognizing.</p><h4 id="_2-dataset" tabindex="-1">2. Dataset <a class="header-anchor" href="#_2-dataset" aria-hidden="true">#</a></h4><p>The data using in this research is taken from <a href="https://www.kaggle.com/msambare/fer2013" target="_blank" rel="noopener noreferrer">Kaggle&#39;s FER-2013</a>.</p><blockquote><p>The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.</p></blockquote><p>However, in order to get a sufficient data for my model training, as well as avoiding imbalance between data classes, i just take out 3 most important emotions which are happy, sad and neutral. I also get rid of the testing phase due to the limitation in data. The training set is divided into 2 smaller sets, which are training set (80%) and validation set (20%).</p><h4 id="_3-proposed-architecture" tabindex="-1">3. Proposed architecture <a class="header-anchor" href="#_3-proposed-architecture" aria-hidden="true">#</a></h4><p>The originally proposed VGGFace architecture was shown as: <p align="center"><img src="https://i1.wp.com/sefiks.com/wp-content/uploads/2019/04/vgg-face-architecture.jpg?ssl=1" alt="vggface-architecture"><div align="center"><figcaption><b>Fig 1.</b> Visualization of VGGFace architecture</figcaption></div></p></p><p>However, i did some minor change in the original architecture to give out a better performance to my own problem. In details, there is an extra layer after the second one, an extra dense layer in the fully connected one, the activation function utilized is <a href="https://www.mygreatlearning.com/blog/relu-activation-function/" target="_blank" rel="noopener noreferrer">ReLU</a> and some dropout layer.</p><p>The detailed model is shown as:</p><div class="language-python"><pre><code><span class="token triple-quoted-string string">&#39;&#39;&#39; First layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                input_shape<span class="token operator">=</span><span class="token punctuation">(</span>img_width<span class="token punctuation">,</span> img_height<span class="token punctuation">,</span> img_depth<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Second layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Extra layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Fully connected layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span>kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;softmax&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>Additionally, i also create a callback with a demonstration as: the learning rate would reduce with a factor of 0.5 once 7 continuous epochs do not improve their performance; and an early stopping is set once the performance does not improve in 7 consecutive epochs.</p><div class="language-python"><pre><code>early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    min_delta<span class="token operator">=</span><span class="token number">0.00005</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    restore_best_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

lr_scheduler <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
    min_lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The training process is shown as: <p align="center"><img src="/emo-recog-trainning-process.png" alt="training-process"><div algin="center"></div></p></p><p>We can notice our model just has to go through 43 epochs before coming to the early convergence.</p><p>Moreover, since this model is rather huge and we can not use the whole model to predict in such a limited time when we harness our model in real-time. I would save the weights from model into a json model, and in main program, we just have to load our weights in the json only.</p><div class="language-python"><pre><code>fer_json <span class="token operator">=</span> model<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face-model.json&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;w&quot;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> json_file<span class="token punctuation">:</span>
    json_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>fer_json<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face.h5&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>And voil√†, here is our final result after training the model and you can see its performance exceeds our expectation. <p align="center"><img src="/emo-detect-demo.png" width="80%" alt="demo-project"></p></p><p>Detailed implementation here: <a href="https://github.com/ngctnnnn/RealTime-Emotion-Recognizer" target="_blank" rel="noopener noreferrer">ngctnnnn/RealTime-Emotion-Recognizer</a></p><h4 id="_4-references" tabindex="-1">4. References <a class="header-anchor" href="#_4-references" aria-hidden="true">#</a></h4><p>[1] Qawaqneh et al. (2017). Deep convolutional neural network for age estimation based on VGG-face model. <em>arXiv:1709.01664</em>.</p><p>[2] I. J. Goodfellow et al. Challenges in representation learning: A report on three machine learning contests. <em>Neural Networks, 64:59--63, 2015. Special Issue on &quot;Deep Learning of Representations&quot;</em>.</p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Detect COVID-19 with Deep Learning]]></title>
            <link>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</link>
            <guid>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</guid>
            <pubDate>Mon, 30 Aug 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.    

]]></description>
            <content:encoded><![CDATA[<div><p>Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.</p><hr><h3 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h3><ol><li><a href="#:~:text=1.%20Scientific%20base">Scientific base</a></li><li><a href="#:~:text=2.%20A%20deep%20learning%20based%20approach%20to%20the%20problem">A deep learning based approach to the problem</a></li><li><a href="#:~:text=3.%20Experimental%20results%20and%20evaluation">Experimental results and evaluation</a></li><li><a href="#:~:text=4.%20References">References</a></li></ol><hr><h4 id="_1-scientific-base" tabindex="-1">1. Scientific base <a class="header-anchor" href="#_1-scientific-base" aria-hidden="true">#</a></h4><p>The most crucial thing that make CXR images from pneumonia or COVID-19 patients different from normal ones is the appearance of white spots, whether they are a lot of or a few, on particular positions along patients&#39; lungs. Those white spots are recognized as the term of <em><strong>ground glass opacity</strong></em> or GGO in medical science. Ground glass opacity is the incompletely consolidated injury in patients&#39; lungs. It has a higher density in comparison with surrounded parenchyma while still enables us to observe underlying structures, e.g. blood vessels or bronchial membranes.</p><p align="center"><img src="/covid+pneumonia+normal.png" alt="ground-glass-pattern image"><div align="center"><figcaption><b>Fig 1.</b> Representative CXR images for 3 cases</figcaption><figcaption>COVID-19 (A), Pneumonia (B) and non-respiratory disease (C)</figcaption></div></p><p>A specified doctor in the field of diagnostic imaging could tell that those GGO is the reason for those white spots in the chest radiograph. And a professional radiologist could use these features to differentiate COVID19 with pneumonia patients. Thus, we are capable of using a deep learning network to extract these features, then categorize to give out the appropriate diagnostic results for every cases.</p><h4 id="_2-a-deep-learning-based-approach-to-the-problem" tabindex="-1">2. A deep learning based approach to the problem <a class="header-anchor" href="#_2-a-deep-learning-based-approach-to-the-problem" aria-hidden="true">#</a></h4><p>Throughout the research, we harness the use of 2 different approaches which are <b>ResNet50</b> and <b>VGG19</b> to solve this problem. In addtion, we use <a href="https://github.com/lindawangg/COVID-Net/blob/master/docs/COVIDx.md" target="_blank" rel="noopener noreferrer">COVIDx dataset</a> - which is a widely used dataset in recent research about COVID-19 nowadays.</p><p>VGG19 is a deep neural network architecture under-using residual design principals, it is also a compact architecture which has a low diversity of architectures. On the other hand, ResNet50 is a deep neural network harnessing residual design principles and it has a moderate diversity of architectures. This network brings many a high productivity in a large number of researching in classifying X-ray images.</p><h4 id="_2-1-covidx-dataset" tabindex="-1">2.1 COVIDx Dataset <a class="header-anchor" href="#_2-1-covidx-dataset" aria-hidden="true">#</a></h4><p>COVIDx Datset is a dataset synthesized from 5 different sources. Additionally, this dataset also provides an image extension transfer tool: from <code>.mri</code> into <code>.jpg</code>. And the author moreover provide a code to support data pre-processing and getting rid of unnecessary part for synthesized data.</p><p>The dataset consists of more than 20.000 CXR images from different patients and divided into 2 sets which are training set and testing set. They are also separated into 3 classes which are pneumonia (train: 5963, test: 105), COVID-19 (train: 4649, test: 274) and the healthy (train: 8751, test: 100).</p><p>Our model will get an input of one CXR image and will give out an output as the probability of that image falling into each class which is pneumonia, COVID-19 and healthy, respectively.</p><h4 id="_2-2-detailed-implementation" tabindex="-1">2.2 Detailed implementation <a class="header-anchor" href="#_2-2-detailed-implementation" aria-hidden="true">#</a></h4><p>Both deep learning neural network we proposed which are VGG19 and ResNet50 are all pre-trained on <a href="https://www.image-net.org/" target="_blank" rel="noopener noreferrer">ImageNet</a>. Afterwards, we proceed training process on COVIDx dataset with Adam as the optimization algorithm and the learning rate&#39;s strategy as reducing if the loss on validation set does not improve at all in a long period (patience).</p><p>Detailed implementation: <a href="https://github.com/ngctnnnn/Detect-COVID19" target="_blank" rel="noopener noreferrer">ngctnnnn/Detect-COVID19</a>.</p><p>After implementation, here is my demo for this project:</p><div align="center"><iframe width="560" height="315" src="/demo-covid19.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><h4 id="_3-experimental-results-and-evaluation" tabindex="-1">3. Experimental results and evaluation <a class="header-anchor" href="#_3-experimental-results-and-evaluation" aria-hidden="true">#</a></h4><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.99</td><td style="text-align:center;">0.82</td><td style="text-align:center;">0.90</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.7</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.81</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.8</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.83</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 1. </b>Results on VGG19</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.97</td><td style="text-align:center;">0.67</td><td style="text-align:center;">0.79</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.56</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.71</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.74</td><td style="text-align:center;">0.85</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 2. </b>Results on ResNet50 (14 epochs)</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.88</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.73</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.79</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.71</td><td style="text-align:center;">0.90</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 3. </b>Results on ResNet50 (50 epochs)</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">96%</td><td style="text-align:center;">86%</td><td style="text-align:center;">82%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">96%</td><td style="text-align:center;">85%</td><td style="text-align:center;">67%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">86%</td><td style="text-align:center;">90%</td><td style="text-align:center;">80%</td></tr></tbody></table><div align="center"><b>Table 4. </b>Comparison among models based on sensitivity</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">70%</td><td style="text-align:center;">80%</td><td style="text-align:center;">99%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">56%</td><td style="text-align:center;">74%</td><td style="text-align:center;">97%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">73%</td><td style="text-align:center;">71%</td><td style="text-align:center;">96%</td></tr></tbody></table><div align="center"><b>Table 5. </b>Comparison among models based on PPV</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Number of parameters (M)</th><th style="text-align:center;">Accuracy</th><th style="text-align:center;">Resolution</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">29.76 trainable + 20.25 non-trainable</td><td style="text-align:center;">86%</td><td style="text-align:center;">480 x 480</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">77%</td><td style="text-align:center;">224 x 224</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">84%</td><td style="text-align:center;">224 x 224</td></tr></tbody></table><div align="center"><b>Table 6. </b>Comparison between precision and number of parameters among models</div><h4 id="_4-references" tabindex="-1">4. References <a class="header-anchor" href="#_4-references" aria-hidden="true">#</a></h4><p>[1] A. Chung. <em>Actualmed covid-19 chest x-ray data initiative</em>, 2020, URL: <a href="https://github.com/agchung/Actualmed-COVID-chestxray-dataset" target="_blank" rel="noopener noreferrer">https://ncov.moh.gov.vn</a>.</p><p>[2] J. P. Cohen et al. <em>Covid-19 image data collection</em>, 2020.</p><p>[3] J. Deng et al. <em>Imagenet: A large-scale hierarchicalimage database</em>. In2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255.Ieee, 2009.</p><p>[4] K. He et al. <em>Deep residual learning for image recognition</em>, 2015.</p><p>[5] Ministry of Health of Vietnam. <em>A page about acute respiratory tract infections covid-19</em>, 2021. URL: <a href="https://ncov.moh.gov.vn" target="_blank" rel="noopener noreferrer">https://ncov.moh.gov.vn</a>.</p><p>[6] Radiological Society of North America. <em>Covid-19 radiography database</em>, 2019. URL: <a href="https://www.kaggle.com/tawsifurrahman/covid19-radiography-database" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/tawsifurrahman/covid19-radiography-database</a>.</p><p>[7] Radiological Society of North America. <em>RSNA pneumonia detection challenge</em>, 2019. URL: <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data</a>.</p><p>[8] K. Simonyan and A. Zisserman. <em>Very deep convolutional networks for large-scale image recognition</em>, 2015.</p><p>[9] L. Wang, Z. Q. Lin, and A. Wong. <em>Covid-net: a tailored deep convolutional neural network de-sign for detection of covid-19 cases from chest x-ray images.Scientific Reports</em>, 10(1):19549,Nov 2020. ISSN 2045-2322. doi: 10.1038/s41598-020-76550-z. URL: <a href="https://doi.org/10.1038/s41598-020-76550-z" target="_blank" rel="noopener noreferrer">https://doi.org/10.1038/s41598-020-76550-z</a>.</p></div>]]></content:encoded>
        </item>
    </channel>
</rss>