<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>A technical blog</title>
        <link>https://blog.vuejs.org</link>
        <description>The offical technical blog by Tan Ngoc Pham</description>
        <lastBuildDate>Tue, 21 Sep 2021 14:34:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <image>
            <title>A technical blog</title>
            <url>https://vuejs.org/images/logo.png</url>
            <link>https://blog.vuejs.org</link>
        </image>
        <item>
            <title><![CDATA[Leukocyte classification to predict diseases]]></title>
            <link>https://blog.vuejs.org/posts/CNN-for-leukocyte-prediction.html</link>
            <guid>https://blog.vuejs.org/posts/CNN-for-leukocyte-prediction.html</guid>
            <pubDate>Wed, 22 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[The power of applied computer vision has been proved to be extraordinarily effective on many fields and medical is one of those. In this research, i used Convolutional Neural Network as a tool to predict diseases via classifying blood cells to count leukocyte in the respiratory system.

]]></description>
            <content:encoded><![CDATA[<div><p>The power of applied computer vision has been proved to be extraordinarily effective on many fields and medical is one of those. In this research, i used Convolutional Neural Network as a tool to predict diseases via classifying blood cells to count leukocyte in the respiratory system.</p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><h4 id="_2-dataset" tabindex="-1">2. Dataset <a class="header-anchor" href="#_2-dataset" aria-hidden="true">#</a></h4><p>The <a href="http://users.cecs.anu.edu.au/~hrezatofighi/Data/Leukocyte%20Data.htm" target="_blank" rel="noopener noreferrer">LISC - <em>Leukocyte Images for Segmentation and Classification</em></a> has been used for automatic identification and counting of blood cells</p><p>Samples were taken from peripheral blood of 8 normal subjects and 400 samples were obtained from 100 microscope slides. The microscope slides were smeared and stained by Gismo-Right technique and images were acquired by a light microscope (Microscope-Axioskope 40) from the stained peripheral blood using an achromatic lens with a magnification of 100. Then, these images were recorded by a digital camera (Sony Model No. SSCDC50AP) and were saved in the BMP format. The images contain 720Ã—576 pixels. All of them are color images and were collected from Hematology-Oncology and BMT Research Center of Imam Khomeini hospital in Tehran, Iran. The images were classified by a hematologist into normal leukocytes: <strong>basophil</strong>, <strong>eosinophil</strong>, <strong>lymphocyte</strong>, <strong>monocyte</strong>, and <strong>neutrophil</strong>. Also, the areas related to the nucleus and cytoplasm were manually segmented by an expert.</p><h4 id="_3-scientific-base" tabindex="-1">3. Scientific base <a class="header-anchor" href="#_3-scientific-base" aria-hidden="true">#</a></h4><h4 id="_4-model-construction" tabindex="-1">4. Model construction <a class="header-anchor" href="#_4-model-construction" aria-hidden="true">#</a></h4><h4 id="_5-conclusion" tabindex="-1">5. Conclusion <a class="header-anchor" href="#_5-conclusion" aria-hidden="true">#</a></h4></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Real-time emotion recognizing]]></title>
            <link>https://blog.vuejs.org/posts/emotion-recognizing.html</link>
            <guid>https://blog.vuejs.org/posts/emotion-recognizing.html</guid>
            <pubDate>Mon, 20 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Introduce a deep learning approach to the problem of recognizing emotions in real time. 
<p align="center">
  <img width=400px height=400px src="/demo-emotion-recognizing.png" alt="one-piece-logo">
</p>

]]></description>
            <content:encoded><![CDATA[<div><p>Introduce a deep learning approach to the problem of recognizing emotions in real time. <p align="center"><img width="400px" height="400px" src="/demo-emotion-recognizing.png" alt="one-piece-logo"></p></p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ol><li><a href="#1-introduction">Introduction</a></li><li><a href="#2-dataset">Dataset</a></li><li><a href="#3-proposed-architecture">Proposed architecture</a></li></ol><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><p>There is a large number of neural networks nowadays to help us in almost every aspect of our life. In addition, we realize that different problems often require different types of networks. In this problem, I choose to use VGGFace network, or it is also called as Deep Face architecture.</p><p>VGGFace architecture was first introduced to solve the problem of recognizing humans&#39; face (i think it is called as Deep Face at first due to that reason, in my opinion).</p><div align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/IQ9qnqSi3gc" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><p>With that reason in mind, i think that VGGFace would perform well on other problems relevant to our faces as in the problem of emotion recognizing.</p><h4 id="_2-dataset" tabindex="-1">2. Dataset <a class="header-anchor" href="#_2-dataset" aria-hidden="true">#</a></h4><p>The data using in this research is taken from <a href="https://www.kaggle.com/msambare/fer2013" target="_blank" rel="noopener noreferrer">Kaggle&#39;s FER-2013</a>.</p><blockquote><p>The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.</p></blockquote><p>However, in order to get a sufficient data for my model training, as well as avoiding inbalance between data classes, i just take out 3 most important emotions which are happy, sad and neutral. I also get rid of the testing phase due to the limitation in data. The training set is splitted into 2 smaller sets, which are training set (80%) and validation set (20%).</p><h4 id="_3-proposed-architecture" tabindex="-1">3. Proposed architecture <a class="header-anchor" href="#_3-proposed-architecture" aria-hidden="true">#</a></h4><p>The originally proposed VGGFace architecture was shown as: <p align="center"><img src="https://i1.wp.com/sefiks.com/wp-content/uploads/2019/04/vgg-face-architecture.jpg?ssl=1" alt="vggface-architecture"><div algin="center"><figcaption><b>Fig 1.</b> Visualization of VGGFace architecture</figcaption></div></p></p><p>However, i did some minor change in the original architecture to give out a better performance to my own problem. In details, there is an extra layer after the second one, an extra dense layer in the fully connected one, the activation function utilized is <a href="https://www.mygreatlearning.com/blog/relu-activation-function/" target="_blank" rel="noopener noreferrer">ReLU</a> and some dropout layer.</p><p>The detailed model is shown as:</p><div class="language-python"><pre><code><span class="token triple-quoted-string string">&#39;&#39;&#39; First layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                input_shape<span class="token operator">=</span><span class="token punctuation">(</span>img_width<span class="token punctuation">,</span> img_height<span class="token punctuation">,</span> img_depth<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Second layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Extra layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Fully connected layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span>kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;softmax&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>Additionally, i also create a callback with a demonstration as: the learning rate would reduce with a factor of 0.5 once 7 continuous epochs do not improve their performance; and an early stopping is set once the performance does not improve in 7 consecutive epochs.</p><div class="language-python"><pre><code>early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    min_delta<span class="token operator">=</span><span class="token number">0.00005</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    restore_best_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

lr_scheduler <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
    min_lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The training process is shown as: <p align="center"><img src="/emo-recog-trainning-process.png" alt="training-process"><div algin="center"></div></p></p><p>We can notice our model just has to go through 43 epochs before coming to the early convergence.</p><p>Moreover, since this model is rather huge and we can not use the whole model to predict in such a limited time when we harness our model in real-time. I would save the weights from model into a json model, and in main program, we just have to load our weights in the json only.</p><div class="language-python"><pre><code>fer_json <span class="token operator">=</span> model<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face-model.json&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;w&quot;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> json_file<span class="token punctuation">:</span>
    json_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>fer_json<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face.h5&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>And voilÃ , here is our final result after training the model.<br><p align="center"><img src="/demo-emotion-recognizing.png" alt="demo-project"><div algin="center"></div></p></p><p>Detailed implementation here: <a href="https://github.com/ngctnnnn/RealTime-Emotion-Recognizer" target="_blank" rel="noopener noreferrer">ngctnnnn/RealTime-Emotion-Recognizer</a></p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Detect COVID-19 with Deep Learning]]></title>
            <link>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</link>
            <guid>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</guid>
            <pubDate>Mon, 30 Aug 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.    

]]></description>
            <content:encoded><![CDATA[<div><p>Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.</p><hr><h3 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h3><ol><li><a href="#1-scientific-base">Scientific base</a></li><li><a href="#2-a-deep-learning-based-approach-to-the-problem">A deep learning based approach to the problem</a></li><li><a href="#3-experimental-results-and-evaluation">Experimental results and evaluation</a></li></ol><hr><h4 id="_1-scientific-base" tabindex="-1">1. Scientific base <a class="header-anchor" href="#_1-scientific-base" aria-hidden="true">#</a></h4><p>The most crucial thing that make CXR images from pneumonia or COVID-19 patients different from normal ones is the appearance of white spots, whether they are a lot of or a few, on particular positions along patients&#39; lungs. Those white spots are recognized as the term of <em><strong>ground glass opacity</strong></em> or GGO in medical science. Ground glass opacity is the incompletely consolidated injury in patients&#39; lungs. It has a higher density in comparison with surrounded parenchyma while still enables us to observe underlying structures, e.g. blood vessels or bronchial membranes.</p><p align="center"><img src="/covid+pneumonia+normal.png" alt="ground-glass-pattern image"><div algin="center"><figcaption><b>Fig 1.</b> Representative CXR images for 3 cases</figcaption><figcaption>COVID-19 (A), Pneumonia (B) and non-respiratory disease (C)</figcaption></div></p><p>A specified doctor in the field of diagnostic imaging could tell that those GGO is the reason for those white spots in the chest radiograph. And a professional radiologist could use these features to differentiate COVID19 with pneumonia patients. Thus, we are capable of using a deep learning network to extract these features, then categorize to give out the appropriate diagnostic results for every cases.</p><h4 id="_2-a-deep-learning-based-approach-to-the-problem" tabindex="-1">2. A deep learning based approach to the problem <a class="header-anchor" href="#_2-a-deep-learning-based-approach-to-the-problem" aria-hidden="true">#</a></h4><p>Throughout the research, we harness the use of 2 different approaches which are <b>ResNet50</b> and <b>VGG19</b> to solve this problem. In addtion, we use <a href="https://github.com/lindawangg/COVID-Net/blob/master/docs/COVIDx.md" target="_blank" rel="noopener noreferrer">COVIDx dataset</a> - which is a widely used dataset in recent research about COVID-19 nowadays.</p><p>VGG19 is a deep neural network architecture under-using residual design principals, it is also a compact architecture which has a low diversity of architectures. On the other hand, ResNet50 is a deep neural network harnessing residual design principles and it has a moderate diversity of architectures. This network brings many a high productivity in a large number of researching in classifying X-ray images.</p><h4 id="_2-1-covidx-dataset" tabindex="-1">2.1 COVIDx Dataset <a class="header-anchor" href="#_2-1-covidx-dataset" aria-hidden="true">#</a></h4><p>COVIDx Datset is a dataset synthesized from 5 different sources. Additionally, this dataset also provides an image extension transfer tool: from <code>.mri</code> into <code>.jpg</code>. And the author moreover provide a code to support data pre-processing and getting rid of unnecessary part for synthesized data.</p><p>The dataset consists of more than 20.000 CXR images from different patients and divided into 2 sets which are training set and testing set. They are also separated into 3 classes which are pneumonia (train: 5963, test: 105), COVID-19 (train: 4649, test: 274) and the healthy (train: 8751, test: 100).</p><p>Our model will get an input of one CXR image and will give out an output as the probability of that image falling into each class which is pneumonia, COVID-19 and healthy, respectively.</p><h4 id="_2-2-detailed-implementation" tabindex="-1">2.2 Detailed implementation <a class="header-anchor" href="#_2-2-detailed-implementation" aria-hidden="true">#</a></h4><p>Both deep learning neural network we proposed which are VGG19 and ResNet50 are all pre-trained on <a href="https://www.image-net.org/" target="_blank" rel="noopener noreferrer">ImageNet</a>. Afterwards, we proceed training process on COVIDx dataset with Adam as the optimization algorithm and the learning rate&#39;s strategy as reducing if the loss on validation set does not improve at all in a long period (patience).</p><p>Detailed implementation: <a href="https://github.com/ngctnnnn/Detect-COVID19" target="_blank" rel="noopener noreferrer">ngctnnnn/Detect-COVID19</a>.</p><p>After implementation, here is my demo for this project:</p><div align="center"><iframe width="560" height="315" src="../public/demo-covid19.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><h4 id="_3-experimental-results-and-evaluation" tabindex="-1">3. Experimental results and evaluation <a class="header-anchor" href="#_3-experimental-results-and-evaluation" aria-hidden="true">#</a></h4><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.99</td><td style="text-align:center;">0.82</td><td style="text-align:center;">0.90</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.7</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.81</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.8</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.83</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 1. </b>Results on VGG19</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.97</td><td style="text-align:center;">0.67</td><td style="text-align:center;">0.79</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.56</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.71</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.74</td><td style="text-align:center;">0.85</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 2. </b>Results on ResNet50 (14 epochs)</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.88</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.73</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.79</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.71</td><td style="text-align:center;">0.90</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 3. </b>Results on ResNet50 (50 epochs)</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">96%</td><td style="text-align:center;">86%</td><td style="text-align:center;">82%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">96%</td><td style="text-align:center;">85%</td><td style="text-align:center;">67%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">86%</td><td style="text-align:center;">90%</td><td style="text-align:center;">80%</td></tr></tbody></table><div align="center"><b>Table 4. </b>Comparison among models based on sensitivity</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">70%</td><td style="text-align:center;">80%</td><td style="text-align:center;">99%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">56%</td><td style="text-align:center;">74%</td><td style="text-align:center;">97%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">73%</td><td style="text-align:center;">71%</td><td style="text-align:center;">96%</td></tr></tbody></table><div align="center"><b>Table 5. </b>Comparison among models based on PPV</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Number of parameters (M)</th><th style="text-align:center;">Accuracy</th><th style="text-align:center;">Resolution</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">29.76 trainable + 20.25 non-trainable</td><td style="text-align:center;">86%</td><td style="text-align:center;">480 x 480</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">77%</td><td style="text-align:center;">224 x 224</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">84%</td><td style="text-align:center;">224 x 224</td></tr></tbody></table><div align="center"><b>Table 6. </b>Comparison between precision and number of parameters among models</div></div>]]></content:encoded>
        </item>
    </channel>
</rss>