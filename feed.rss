<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>A technical blog</title>
        <link>https://blog.vuejs.org</link>
        <description>The offical technical blog by Tan Ngoc Pham</description>
        <lastBuildDate>Thu, 23 Sep 2021 04:04:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <image>
            <title>A technical blog</title>
            <url>https://vuejs.org/images/logo.png</url>
            <link>https://blog.vuejs.org</link>
        </image>
        <item>
            <title><![CDATA[Leukocyte classification to predict diseases (Part 1)]]></title>
            <link>https://blog.vuejs.org/posts/CNN-for-leukocyte-prediction.html</link>
            <guid>https://blog.vuejs.org/posts/CNN-for-leukocyte-prediction.html</guid>
            <pubDate>Wed, 22 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Data analysis on number of blood cells, which are white blood cells and red blood cells in details, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.

]]></description>
            <content:encoded><![CDATA[<div><p>Data analysis on number of blood cells, which are white blood cells and red blood cells in details, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.</p><hr><p>Since this blog is rather long, we divide it into 2 parts, the first one is to introduce our approach to the problem, our dataset and our approach to preprocess data, then the second one is to discuss further about how to identify our needed cells in the preprocessed data through Canny object detection and Hough transformation.</p><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ol><li><a href="#1-introduction">Introduction</a></li><li><a href="#2-dataset-and-data-preprocessing">Dataset and data preprocessing</a></li><li><a href="#3-foreground-and-background-segmentation-technique">Foreground and background segmentation technique</a></li><li><a href="#4-edge-detection-with-Canny-method">Edge detection with Canny method</a></li><li><a href="#5-hough">Hough transformation to identify blood cells borderlines</a></li><li><a href="#6-references">References</a></li></ol><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><p>While blood cells (WBC), also known as <em>leukocytes</em>, are produced in the bone marrow and composed of nuclei and cytoplasm. WBCs are divided into five groups: basophil, eosinophil, lymphocyte, monocyte and neutrophil. Leukocytes protects the body against infectious disease and foreign substance, constitute an important part of the immune system. 4.1e9 to 11.1e9 units in one liter of a healthy adult human is the normal number of WBCs in a blood is 4,500 to 11,000 WBCs per microliter and a drop in a blood is 7,000 to 25,000.</p><div align="center"><table><thead><tr><th><p align="center"> Name</p></th><th><p align="center"> Description </p></th></tr></thead><tbody><tr><td><p align="center"> Neutrophils </p></td><td><p align="center"> Contact the microbial invasion, phagocytize and destroy invading organism </p></td></tr><tr><td><p align="center"> Eosinophils </p></td><td><p align="center"> Part of defense mechanism </p><p align="center">against parasitic infections, inflammatory processes and allergic tissue reactions </p></td></tr><tr><td><p align="center"> Basophils </p></td><td><p align="center"> Play role in allergic and immediate hypersensitivity reactions </p></td></tr><tr><td><p align="center"> Monocytes </p></td><td><p align="center"> Involve in defensive reactions to some microorganisms, remove damaged cells, cell debris</p><p align="center"> and armour bactericidal action as immune reaction </p></td></tr><tr><td><p align="center"> Lymphocytes </p></td><td><p align="center"> Produce antibodies and join in immune reactions </p></td></tr></tbody></table><p><b>Table 1. </b>Names and brief demonstration of normal leukocytes.</p></div><h4 id="_2-dataset-and-data-preprocessing" tabindex="-1">2. Dataset and data preprocessing <a class="header-anchor" href="#_2-dataset-and-data-preprocessing" aria-hidden="true">#</a></h4><p>The <a href="http://users.cecs.anu.edu.au/~hrezatofighi/Data/Leukocyte%20Data.htm" target="_blank" rel="noopener noreferrer">LISC - <em>Leukocyte Images for Segmentation and Classification</em></a> is used for automatic identification and blood cells counting since it is relatively easy to use.</p><blockquote><p>Samples were taken from peripheral blood of 8 normal subjects and 400 samples were obtained from 100 microscope slides. The microscope slides were smeared and stained by Gismo-Right technique and images were acquired by a light microscope (Microscope-Axioskope 40) from the stained peripheral blood using an achromatic lens with a magnification of 100. Then, these images were recorded by a digital camera and were saved in the BMP format. The images contain 720×576 pixels.</p></blockquote><blockquote><p>All of them are color images. The images were classified by a hematologist into normal leukocytes: basophil, eosinophil, lymphocyte, monocyte, and neutrophil. Also, the areas related to the nucleus and cytoplasm were manually segmented by an expert.</p></blockquote><p>You can download dataset here: <a href="http://users.cecs.anu.edu.au/~hrezatofighi/Data/Leukocyte%20Data.htm" target="_blank" rel="noopener noreferrer">LISC database</a>.</p><p>After that, images are transformed into gray scale using Python&#39;s <a href="https://pypi.org/project/opencv-python/" target="_blank" rel="noopener noreferrer">opencv</a> library:</p><div class="language-python"><pre><code><span class="token keyword">import</span> cv2
gray_img <span class="token operator">=</span> cv2<span class="token punctuation">.</span>cvtColor<span class="token punctuation">(</span>original_img<span class="token punctuation">,</span> cv2<span class="token punctuation">.</span>COLOR_BGR2GRAY<span class="token punctuation">)</span>
</code></pre></div><p>And after using opencv, we can change a normal leukocyte into a grayscale leukocyte.</p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><img width="70%" src="https://github.com/BTrDung/Complex/raw/master/CreProjCBC/4.bmp" alt="leukocyte-before"></div><div><img width="40%" src="/grayscale-leukocyte.png" alt="leukocyte-after"></div></div><div align="center"><b>Fig 1.</b> Image of a normal leukocyte before and after using opencv </div><h4 id="_3-foreground-and-background-segmentation-technique" tabindex="-1">3. Foreground and background segmentation technique <a class="header-anchor" href="#_3-foreground-and-background-segmentation-technique" aria-hidden="true">#</a></h4><p>At the very first glance to the solution, we have to notice how to differentiate between foreground and background.</p><p>We could define foreground as an object that we want to observe detailedly. For a thorough understanding, a foregrounding object is the object we would like to separate from the whole picture for later purposes, e.g. object detecting or motion predicting. In this topic area, leukocytes and erythrocytes are defined as the foreground and moreover, we will not use background. Hence, at the end this section, we would guide you thoroughly to extract the foreground from the a whole picture.</p><p>Let make an overview about the image mentioned below:</p><p align="center"><img src="https://webpath.med.utah.edu/jpeg5/HEME005.jpg" alt="basophil"><div algin="center"><figcaption><b>Fig 2.</b> Example of one blood cell photo from dataset </figcaption></div></p><p>Scientific evidence has it that dark purple areas are leukocytes and light purple areas are erythrocytes. We would divide the whole picture into 2 different ones: the first one only consists of leukocytes&#39; positions and the other consists of those from erythrocytes; and both of them would remain the original scale. The reason behind doing splitting thing is to make processing and classifying procedure more easily.</p><p>One of the most basic approach for this method is to use thresholding technique. The threshold formula could be expressed as below:</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;f(x,y)&amp;space;=&amp;space;\left\{\begin{matrix}&amp;space;255&amp;space;&amp;f(x,y)&amp;space;&gt;&amp;space;thres&amp;space;\\&amp;space;0&amp;space;&amp;f(x,y)\leq&amp;space;thres&amp;space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{150}&amp;space;f(x,y)&amp;space;=&amp;space;\left\{\begin{matrix}&amp;space;255&amp;space;&amp;f(x,y)&amp;space;&gt;&amp;space;\theta&amp;space;\\&amp;space;0&amp;space;&amp;f(x,y)\leq&amp;space;\theta&amp;space;\end{matrix}\right." title="f(x,y) = \left\{\begin{matrix} 255 &amp;f(x,y) &gt; \theta \\ 0 &amp;f(x,y)\leq thres \end{matrix}\right."></a> s.t. <em><strong>θ</strong></em> is defined as our threshold</p><p>Since our image has been already transformed from RGB to gray-scale, all pixels are now in the threshold of from 0 to 255. We could get rid of background part by using <a href="https://www.investopedia.com/terms/h/histogram.asp" target="_blank" rel="noopener noreferrer"><em>histogram</em></a>, which is the graph for representation of the distribution of colors in an image, to analyse and choose an appropriate threshold afterwards (i recommend you could take the chosen threshold from the number with the highest wave frequency).</p><p align="center"><img width="80%" src="/histogram.png" alt="histogram"><div align="center"><figcaption><b>Fig 3.</b> Histogram of Figure 1 (threshold = 255). </figcaption></div></p><p>After that, the area with pixels valuing in <code>f(x,y) &gt; threshold</code> are categorized as the foreground and vice versa is the background to identify the erythrocytes. In addition, we also apply another formula to extract leukocytes from the total image, which is:</p><p><a href="https://www.codecogs.com/eqnedit.php?latex=\inline&amp;space;\dpi{150}&amp;space;f(x,&amp;space;y)=\left\{\begin{matrix}&amp;space;255&amp;space;&amp;threshold_{min}&amp;space;\leq&amp;space;f(x,y)&amp;space;\leq&amp;space;threshold_{max}\\&amp;space;0&amp;space;&amp;f(x,&amp;space;y)&amp;space;\notin&amp;space;\[threshold_{min},threshold_{max}\]&amp;space;\end{matrix}\right." target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&amp;space;\dpi{150}&amp;space;f(x,&amp;space;y)=\left\{\begin{matrix}&amp;space;255&amp;space;&amp;\theta_{min}&amp;space;\leq&amp;space;f(x,y)&amp;space;\leq&amp;space;\theta_{max}\\&amp;space;0&amp;space;&amp;f(x,&amp;space;y)&amp;space;\notin&amp;space;\[\theta_{min},\theta_{max}\]&amp;space;\end{matrix}\right." title="f(x, y)=\left\{\begin{matrix} 255 &amp;threshold_{min} \leq f(x,y) \leq threshold_{max}\\ 0 &amp;f(x, y) \notin \[threshold_{min},threshold_{max}\] \end{matrix}\right."></a></p><p>After using our proposed method, we would achieve the result as followed:</p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><p align="center"><img width="80%" src="/WBCs.png" alt="White blood cells image after using threshold"><div align="center"><figcaption><b>Fig 4.</b> Leukocytes after categorized as foreground </figcaption></div></p></div><div><p align="center"><img width="80%" src="/RBC.png" alt="Red blood cells image after using threshold"><div align="center"><figcaption><b>Fig 5.</b> Erythrocytes after categorized as foreground </figcaption></div></p></div></div><p>Our next blog would discuss further about how to detect and count blood cells in preprocessed images.</p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Real-time emotion recognizing]]></title>
            <link>https://blog.vuejs.org/posts/emotion-recognizing.html</link>
            <guid>https://blog.vuejs.org/posts/emotion-recognizing.html</guid>
            <pubDate>Mon, 20 Sep 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Introduce a deep learning approach to the problem of recognizing emotions in real time. 
<p align="center">
  <img width=400px height=100% src="/demo-emotion-recognizing.png" alt="demo-emotion">
</p>

]]></description>
            <content:encoded><![CDATA[<div><p>Introduce a deep learning approach to the problem of recognizing emotions in real time. <p align="center"><img width="400px" height="100%" src="/demo-emotion-recognizing.png" alt="demo-emotion"></p></p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ol><li><a href="#1-introduction">Introduction</a></li><li><a href="#2-dataset">Dataset</a></li><li><a href="#3-proposed-architecture">Proposed architecture</a></li><li><a href="#4-references">References</a></li></ol><hr><h4 id="_1-introduction" tabindex="-1">1. Introduction <a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a></h4><p>There is a large number of neural networks nowadays to help us in almost every aspect of our life. In addition, we realize that different problems often require different types of networks. In this problem, I choose to use VGGFace network, or it is also called as Deep Face architecture.</p><p>VGGFace architecture was first introduced to solve the problem of recognizing humans&#39; face (i think it is called as Deep Face at first due to that reason, in my opinion).</p><div align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/IQ9qnqSi3gc" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><p>With that reason in mind, i think that VGGFace would perform well on other problems relevant to our faces as in the problem of emotion recognizing.</p><h4 id="_2-dataset" tabindex="-1">2. Dataset <a class="header-anchor" href="#_2-dataset" aria-hidden="true">#</a></h4><p>The data using in this research is taken from <a href="https://www.kaggle.com/msambare/fer2013" target="_blank" rel="noopener noreferrer">Kaggle&#39;s FER-2013</a>.</p><blockquote><p>The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.</p></blockquote><p>However, in order to get a sufficient data for my model training, as well as avoiding imbalance between data classes, i just take out 3 most important emotions which are happy, sad and neutral. I also get rid of the testing phase due to the limitation in data. The training set is divided into 2 smaller sets, which are training set (80%) and validation set (20%).</p><h4 id="_3-proposed-architecture" tabindex="-1">3. Proposed architecture <a class="header-anchor" href="#_3-proposed-architecture" aria-hidden="true">#</a></h4><p>The originally proposed VGGFace architecture was shown as: <p align="center"><img src="https://i1.wp.com/sefiks.com/wp-content/uploads/2019/04/vgg-face-architecture.jpg?ssl=1" alt="vggface-architecture"><div algin="center"><figcaption><b>Fig 1.</b> Visualization of VGGFace architecture</figcaption></div></p></p><p>However, i did some minor change in the original architecture to give out a better performance to my own problem. In details, there is an extra layer after the second one, an extra dense layer in the fully connected one, the activation function utilized is <a href="https://www.mygreatlearning.com/blog/relu-activation-function/" target="_blank" rel="noopener noreferrer">ReLU</a> and some dropout layer.</p><p>The detailed model is shown as:</p><div class="language-python"><pre><code><span class="token triple-quoted-string string">&#39;&#39;&#39; First layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                input_shape<span class="token operator">=</span><span class="token punctuation">(</span>img_width<span class="token punctuation">,</span> img_height<span class="token punctuation">,</span> img_depth<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Second layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Extra layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Conv2D<span class="token punctuation">(</span>filters<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">&#39;same&#39;</span><span class="token punctuation">,</span>
                kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>MaxPool2D<span class="token punctuation">(</span>pool_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token triple-quoted-string string">&#39;&#39;&#39; Fully connected layer &#39;&#39;&#39;</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;relu&#39;</span><span class="token punctuation">,</span>kernel_initializer<span class="token operator">=</span><span class="token string">&#39;he_normal&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>BatchNormalization<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span><span class="token number">0.6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span>activation<span class="token operator">=</span><span class="token string">&#39;softmax&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>Additionally, i also create a callback with a demonstration as: the learning rate would reduce with a factor of 0.5 once 7 continuous epochs do not improve their performance; and an early stopping is set once the performance does not improve in 7 consecutive epochs.</p><div class="language-python"><pre><code>early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    min_delta<span class="token operator">=</span><span class="token number">0.00005</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">11</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    restore_best_weights<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

lr_scheduler <span class="token operator">=</span> ReduceLROnPlateau<span class="token punctuation">(</span>
    monitor<span class="token operator">=</span><span class="token string">&#39;val_accuracy&#39;</span><span class="token punctuation">,</span>
    factor<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>
    patience<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>
    min_lr<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span>
    verbose<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The training process is shown as: <p align="center"><img src="/emo-recog-trainning-process.png" alt="training-process"><div algin="center"></div></p></p><p>We can notice our model just has to go through 43 epochs before coming to the early convergence.</p><p>Moreover, since this model is rather huge and we can not use the whole model to predict in such a limited time when we harness our model in real-time. I would save the weights from model into a json model, and in main program, we just have to load our weights in the json only.</p><div class="language-python"><pre><code>fer_json <span class="token operator">=</span> model<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face-model.json&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;w&quot;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> json_file<span class="token punctuation">:</span>
    json_file<span class="token punctuation">.</span>write<span class="token punctuation">(</span>fer_json<span class="token punctuation">)</span>
model<span class="token punctuation">.</span>save_weights<span class="token punctuation">(</span><span class="token string">&quot;model/vgg-face.h5&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>And voilà, here is our final result after training the model and you can see its performance exceeds our expectation. <p align="center"><img src="/emo-detect-demo.png" width="80%" alt="demo-project"></p></p><p>Detailed implementation here: <a href="https://github.com/ngctnnnn/RealTime-Emotion-Recognizer" target="_blank" rel="noopener noreferrer">ngctnnnn/RealTime-Emotion-Recognizer</a></p><h4 id="_4-references" tabindex="-1">4. References <a class="header-anchor" href="#_4-references" aria-hidden="true">#</a></h4><p>[1] Qawaqneh et al. (2017). Deep convolutional neural network for age estimation based on VGG-face model. <em>arXiv:1709.01664</em>.</p><p>[2] I. J. Goodfellow et al. Challenges in representation learning: A report on three machine learning contests. <em>Neural Networks, 64:59--63, 2015. Special Issue on &quot;Deep Learning of Representations&quot;</em>.</p></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Detect COVID-19 with Deep Learning]]></title>
            <link>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</link>
            <guid>https://blog.vuejs.org/posts/detect-covid19-dcnn.html</guid>
            <pubDate>Mon, 30 Aug 2021 12:00:00 GMT</pubDate>
            <description><![CDATA[Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.    

]]></description>
            <content:encoded><![CDATA[<div><p>Propose a rapidly testing method which has a high productivity in a short time, which is to use Deep Convolutional Neural Network to detect COVID-19 on Chest X-ray (CXR) images to cope with the present pandemic.</p><hr><h3 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h3><ol><li><a href="#1-scientific-base">Scientific base</a></li><li><a href="#2-a-deep-learning-based-approach-to-the-problem">A deep learning based approach to the problem</a></li><li><a href="#3-experimental-results-and-evaluation">Experimental results and evaluation</a></li><li><a href="#4-references">References</a></li></ol><hr><h4 id="_1-scientific-base" tabindex="-1">1. Scientific base <a class="header-anchor" href="#_1-scientific-base" aria-hidden="true">#</a></h4><p>The most crucial thing that make CXR images from pneumonia or COVID-19 patients different from normal ones is the appearance of white spots, whether they are a lot of or a few, on particular positions along patients&#39; lungs. Those white spots are recognized as the term of <em><strong>ground glass opacity</strong></em> or GGO in medical science. Ground glass opacity is the incompletely consolidated injury in patients&#39; lungs. It has a higher density in comparison with surrounded parenchyma while still enables us to observe underlying structures, e.g. blood vessels or bronchial membranes.</p><p align="center"><img src="/covid+pneumonia+normal.png" alt="ground-glass-pattern image"><div algin="center"><figcaption><b>Fig 1.</b> Representative CXR images for 3 cases</figcaption><figcaption>COVID-19 (A), Pneumonia (B) and non-respiratory disease (C)</figcaption></div></p><p>A specified doctor in the field of diagnostic imaging could tell that those GGO is the reason for those white spots in the chest radiograph. And a professional radiologist could use these features to differentiate COVID19 with pneumonia patients. Thus, we are capable of using a deep learning network to extract these features, then categorize to give out the appropriate diagnostic results for every cases.</p><h4 id="_2-a-deep-learning-based-approach-to-the-problem" tabindex="-1">2. A deep learning based approach to the problem <a class="header-anchor" href="#_2-a-deep-learning-based-approach-to-the-problem" aria-hidden="true">#</a></h4><p>Throughout the research, we harness the use of 2 different approaches which are <b>ResNet50</b> and <b>VGG19</b> to solve this problem. In addtion, we use <a href="https://github.com/lindawangg/COVID-Net/blob/master/docs/COVIDx.md" target="_blank" rel="noopener noreferrer">COVIDx dataset</a> - which is a widely used dataset in recent research about COVID-19 nowadays.</p><p>VGG19 is a deep neural network architecture under-using residual design principals, it is also a compact architecture which has a low diversity of architectures. On the other hand, ResNet50 is a deep neural network harnessing residual design principles and it has a moderate diversity of architectures. This network brings many a high productivity in a large number of researching in classifying X-ray images.</p><h4 id="_2-1-covidx-dataset" tabindex="-1">2.1 COVIDx Dataset <a class="header-anchor" href="#_2-1-covidx-dataset" aria-hidden="true">#</a></h4><p>COVIDx Datset is a dataset synthesized from 5 different sources. Additionally, this dataset also provides an image extension transfer tool: from <code>.mri</code> into <code>.jpg</code>. And the author moreover provide a code to support data pre-processing and getting rid of unnecessary part for synthesized data.</p><p>The dataset consists of more than 20.000 CXR images from different patients and divided into 2 sets which are training set and testing set. They are also separated into 3 classes which are pneumonia (train: 5963, test: 105), COVID-19 (train: 4649, test: 274) and the healthy (train: 8751, test: 100).</p><p>Our model will get an input of one CXR image and will give out an output as the probability of that image falling into each class which is pneumonia, COVID-19 and healthy, respectively.</p><h4 id="_2-2-detailed-implementation" tabindex="-1">2.2 Detailed implementation <a class="header-anchor" href="#_2-2-detailed-implementation" aria-hidden="true">#</a></h4><p>Both deep learning neural network we proposed which are VGG19 and ResNet50 are all pre-trained on <a href="https://www.image-net.org/" target="_blank" rel="noopener noreferrer">ImageNet</a>. Afterwards, we proceed training process on COVIDx dataset with Adam as the optimization algorithm and the learning rate&#39;s strategy as reducing if the loss on validation set does not improve at all in a long period (patience).</p><p>Detailed implementation: <a href="https://github.com/ngctnnnn/Detect-COVID19" target="_blank" rel="noopener noreferrer">ngctnnnn/Detect-COVID19</a>.</p><p>After implementation, here is my demo for this project:</p><div align="center"><iframe width="560" height="315" src="/demo-covid19.mp4" frameborder="0" allow="accelerometer; autoplay; encrypted-media gyroscope; picture-in-picture" allowfullscreen></iframe></div><h4 id="_3-experimental-results-and-evaluation" tabindex="-1">3. Experimental results and evaluation <a class="header-anchor" href="#_3-experimental-results-and-evaluation" aria-hidden="true">#</a></h4><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.99</td><td style="text-align:center;">0.82</td><td style="text-align:center;">0.90</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.7</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.81</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.8</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.83</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 1. </b>Results on VGG19</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.97</td><td style="text-align:center;">0.67</td><td style="text-align:center;">0.79</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.56</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.71</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.74</td><td style="text-align:center;">0.85</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 2. </b>Results on ResNet50 (14 epochs)</div><table><thead><tr><th style="text-align:center;">Disease</th><th style="text-align:center;">Precision</th><th style="text-align:center;">Recall</th><th style="text-align:center;">F1-score</th><th style="text-align:center;">Support</th></tr></thead><tbody><tr><td style="text-align:center;">COVID-19</td><td style="text-align:center;">0.96</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.88</td><td style="text-align:center;">274</td></tr><tr><td style="text-align:center;">Non-respiratory disease</td><td style="text-align:center;">0.73</td><td style="text-align:center;">0.86</td><td style="text-align:center;">0.79</td><td style="text-align:center;">100</td></tr><tr><td style="text-align:center;">Pneumonia</td><td style="text-align:center;">0.71</td><td style="text-align:center;">0.90</td><td style="text-align:center;">0.79</td><td style="text-align:center;">105</td></tr></tbody></table><div align="center"><b>Table 3. </b>Results on ResNet50 (50 epochs)</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">96%</td><td style="text-align:center;">86%</td><td style="text-align:center;">82%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">96%</td><td style="text-align:center;">85%</td><td style="text-align:center;">67%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">86%</td><td style="text-align:center;">90%</td><td style="text-align:center;">80%</td></tr></tbody></table><div align="center"><b>Table 4. </b>Comparison among models based on sensitivity</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Non-respiratory disease</th><th style="text-align:center;">Pneumonia</th><th style="text-align:center;">COVID-19</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">70%</td><td style="text-align:center;">80%</td><td style="text-align:center;">99%</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">56%</td><td style="text-align:center;">74%</td><td style="text-align:center;">97%</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">73%</td><td style="text-align:center;">71%</td><td style="text-align:center;">96%</td></tr></tbody></table><div align="center"><b>Table 5. </b>Comparison among models based on PPV</div><table><thead><tr><th style="text-align:center;">Architecture</th><th style="text-align:center;">Number of parameters (M)</th><th style="text-align:center;">Accuracy</th><th style="text-align:center;">Resolution</th></tr></thead><tbody><tr><td style="text-align:center;">VGG19</td><td style="text-align:center;">29.76 trainable + 20.25 non-trainable</td><td style="text-align:center;">86%</td><td style="text-align:center;">480 x 480</td></tr><tr><td style="text-align:center;">ResNet50 (14 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">77%</td><td style="text-align:center;">224 x 224</td></tr><tr><td style="text-align:center;">ResNet50 (50 epochs)</td><td style="text-align:center;">25.93 trainable + 23.77 non-trainable</td><td style="text-align:center;">84%</td><td style="text-align:center;">224 x 224</td></tr></tbody></table><div align="center"><b>Table 6. </b>Comparison between precision and number of parameters among models</div><h4 id="_4-references" tabindex="-1">4. References <a class="header-anchor" href="#_4-references" aria-hidden="true">#</a></h4><p>[1] A. Chung. <em>Actualmed covid-19 chest x-ray data initiative</em>, 2020, URL: <a href="https://github.com/agchung/Actualmed-COVID-chestxray-dataset" target="_blank" rel="noopener noreferrer">https://ncov.moh.gov.vn</a>.</p><p>[2] J. P. Cohen et al. <em>Covid-19 image data collection</em>, 2020.</p><p>[3] J. Deng et al. <em>Imagenet: A large-scale hierarchicalimage database</em>. In2009 IEEE conference on computer vision and pattern recognition, pages 248–255.Ieee, 2009.</p><p>[4] K. He et al. <em>Deep residual learning for image recognition</em>, 2015.</p><p>[5] Ministry of Health of Vietnam. <em>A page about acute respiratory tract infections covid-19</em>, 2021. URL: <a href="https://ncov.moh.gov.vn" target="_blank" rel="noopener noreferrer">https://ncov.moh.gov.vn</a>.</p><p>[6] Radiological Society of North America. <em>Covid-19 radiography database</em>, 2019. URL: <a href="https://www.kaggle.com/tawsifurrahman/covid19-radiography-database" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/tawsifurrahman/covid19-radiography-database</a>.</p><p>[7] Radiological Society of North America. <em>RSNA pneumonia detection challenge</em>, 2019. URL: <a href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data" target="_blank" rel="noopener noreferrer">https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data</a>.</p><p>[8] K. Simonyan and A. Zisserman. <em>Very deep convolutional networks for large-scale image recognition</em>, 2015.</p><p>[9] L. Wang, Z. Q. Lin, and A. Wong. <em>Covid-net: a tailored deep convolutional neural network de-sign for detection of covid-19 cases from chest x-ray images.Scientific Reports</em>, 10(1):19549,Nov 2020. ISSN 2045-2322. doi: 10.1038/s41598-020-76550-z. URL: <a href="https://doi.org/10.1038/s41598-020-76550-z" target="_blank" rel="noopener noreferrer">https://doi.org/10.1038/s41598-020-76550-z</a>.</p></div>]]></content:encoded>
        </item>
    </channel>
</rss>