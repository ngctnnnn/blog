<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Leukocyte classification to predict diseases (Part 2) | A technical blog</title>
    <meta name="description" content="The official blog">
    <link rel="stylesheet" href="/assets/style.a6de2ea5.css">
    
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <meta name="twitter:title" content="Leukocyte classification to predict diseases (Part 2) | A technical blog">
    <meta property="og:title" content="Leukocyte classification to predict diseases (Part 2) | A technical blog">
  </head>
  <body>
    <div id="app"><div class="antialiased"><div class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><nav class="flex justify-between items-center py-10 font-bold"><a class="text-xl" href="/" aria-label="A technical blog"><img class="inline-block mr-2" style="width:36px;height:31px;" alt="logo" src="/logo.svg"><span class="hidden md:inline nav-text">A technical blog</span></a><div class="text-base text-gray-500 leading-5"><a class="hover:text-gray-700" href="https://github.com/ngctnnnn/ngctnnnn.github.io/blob/main/README.md" target="_blank" rel="noopener">About us</a></div></nav></div><main class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0"><article class="xl:divide-y xl:divide-gray-200"><header class="pt-6 xl:pb-10 space-y-1 text-center"><dl><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-500"><time datetime="2021-09-23T12:00:00.000Z">September 23, 2021</time></dd></dl><h1 class="text-3xl leading-9 font-extrabold text-gray-900 tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Leukocyte classification to predict diseases (Part 2)</h1></header><div class="divide-y xl:divide-y-0 divide-gray-200 xl:grid xl:grid-cols-4 xl:gap-x-10 pb-16 xl:pb-20" style="grid-template-rows:auto 1fr;"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200"><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center xl:block space-x-8 sm:space-x-12 xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img src="https://gravatar.com/avatar/9d7fdd037b40f9d989d82eac5c97dd33?s=80" alt="author image" class="w-10 h-10 rounded-full"><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900">Dzung Tri Bui</dd><dt class="sr-only">Linkedin</dt><dd><a href="https://linkedin.com/in/BTrDung" target="_blank" rel="noopnener noreferrer" class="link">@BTrDung</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 xl:pb-0 xl:col-span-3 xl:row-span-2"><div style="position:relative;" class="prose max-w-none pt-10 pb-8"><div><p>Data analysis on number of blood cells, which are white blood cells and red blood cells, per a certain blood volume could help us observe our medical situation. In this blog, we introduce a faster method to recognize disease via the number of leukocytes.</p><hr><h4 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h4><ul><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html" target="_blank" rel="noopener noreferrer">Part 1</a><ul><li><ol><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=1.%20Introduction" target="_blank" rel="noopener noreferrer">Introduction</a></li></ol></li><li><ol start="2"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=2.%20Dataset%20and%20data%20preprocessing" target="_blank" rel="noopener noreferrer">Dataset and data preprocessing</a></li></ol></li><li><ol start="3"><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction.html#:~:text=3.%20Foreground%20and%20background%20segmentation%20technique" target="_blank" rel="noopener noreferrer">Foreground and background segmentation technique</a></li></ol></li></ul></li><li><a href="https://ngctnnnn.github.io/posts/computer-vision-for-leukocyte-prediction-2.html" target="_blank" rel="noopener noreferrer">Part 2</a><ul><li><ol start="4"><li><a href="#:~:text=4.%20Edge%20detection%20with%20Canny%20method">Edge detection with Canny method</a></li></ol></li><li><ol start="5"><li><a href="#:~:text=5.%20Hough%20transformation%20to%20identify%20blood%20cells%20borderlines">Hough transformation to identify blood cells borderlines</a></li></ol></li><li><ol start="6"><li><a href="#:~:text=6.%20References">References</a></li></ol></li></ul></li></ul><hr><h4 id="_4-edge-detection-with-canny-method" tabindex="-1">4. Edge detection with Canny method <a class="header-anchor" href="#_4-edge-detection-with-canny-method" aria-hidden="true">#</a></h4><p>This method uses both high and low as 2 separated thresholds. The high one would be used firstly to find the starting point of the edge. After that, we determine the path direction of the border based on consecutive pixels having a greater value in comparison of the low. Points with the values less than the low threshold are removed only. Tiny borders will be selected if they are associated with large (easy to see) borders. The Canny procedure is demonstrated as follows:</p><ul><li><p><strong>Step 1</strong>: Use a Gaussian filter to smoothen the image. This step is meant to reduce the gradient of pixels while moving from one pixel to another and make the image smoother than the original.</p></li><li><p><strong>Step 2</strong>: Calculate the gradient of the contour of the smoothed image. Computing the gradient is mainly to construct a function to redefine the slope as well as the increasing and decreasing trend of a certain pixel.</p></li><li><p><strong>Step 3</strong>: Remove the non-maxima points. As a side note, we will choose the pixels that are most likely to be considered the highest edge by relying on the gradient calculation in step 2. When a pixel has not reached its peak, we won&#39;t evaluate that pixel as an edge.</p></li><li><p><strong>Step 4</strong>: The final step is to remove the values that are less than the threshold level. Should you observe an image carefully, there are many values after calculating the gradient that will be selected as the maximum value. However, the maximum value here is only local maximum and only a few points are considered as global maximum. Therefore, we will remove some regions in which the local maximum value is lower than the allowable threshold (the threshold value will depend on the type of model to choose more appropriate).</p></li></ul><p>This method is considered as more superior to other methods because it is less affected by noises and it is also able to detect weak edges. On the other hand, if the threshold is chosen too low, the boundary will be incorrect, or if the threshold is chosen too high, much of the important information about the boundary will be discarded. Based on the predefined threshold level, it will decide which points belong to the true boundary or not to the boundary. The lower the threshold level, the more edges are detected (but also noise and false edges appear). Conversely, if the threshold level is set higher, the fuzzy borders may be lost, or the borders will be broken.</p><h4 id="_5-hough-transformation-to-identify-blood-cells-borderlines" tabindex="-1">5. Hough transformation to identify blood cells borderlines <a class="header-anchor" href="#_5-hough-transformation-to-identify-blood-cells-borderlines" aria-hidden="true">#</a></h4><p>Hough transformation is one of the most basic methods for feature extraction in image processing, and especially one of its capabilities is to find and detect circles in images (even imperfect circles).</p><p>This method mainly relies on the parameter accumulator after being added up (called “voting”) to get the maximum values. We will identify objects that are supposed to be circular in image with the following formula:</p><p align="center"><a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{150}&amp;space;r^2&amp;space;=&amp;space;(x&amp;space;-&amp;space;a)^2&amp;space;+&amp;space;(y&amp;space;-&amp;space;b)^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{150}&amp;space;r^2&amp;space;=&amp;space;(x&amp;space;-&amp;space;a)^2&amp;space;+&amp;space;(y&amp;space;-&amp;space;b)^2" title="r^2 = (x - a)^2 + (y - b)^2"></a></p><p>s.t. <strong>r</strong> is radius of circle, <code>(a, b)</code> is the circle&#39;s center.</p><p>The <strong>x</strong>, <strong>y</strong> values only move in the range <code>|r - a| ≤ x ≤ |r + a|</code> and |<code>r - b| ≤ y ≤ |r + b|</code> so that the above formula is satisfied. The implementation of the algorithm to &#39;voting&#39; the positions will be like the setting in determining the circle as above. Here we perform &#39;voting&#39; on a 3D matrix with parameters <strong>r</strong>, <strong>a</strong>, <strong>b</strong>.</p><div align="center" id="banner" style="display:flex;justify-content:space-between;"><div><p align="center"><img width="80%" src="/CircleHoughTransform1.png" alt="Simulate two circles with difference sizes"><div align="center"><figcaption><b>Fig 5.</b> Simulate two circles </figcaption><figcaption>with difference sizes</figcaption></div></p></div><div><p align="center"><img width="80%" src="/CircleHoughTransform2.png" alt="Cumulative table of 2 parameters x, y with fixed r"><div align="center"><figcaption><b>Fig 6.</b> Cumulative table of 2 parameters x, y </figcaption><figcaption> with fixed r</figcaption></div></p></div></div><div align="center"><b>Fig 5,6.</b> Simulating the process of determining a circle using the Hough transform </div><p>For ease of visualization, we draw two small circles. Our task is to help the computer determine the center of two circles. Pay attention to the circle highlighted in blue and you will see that they have a smaller radius than the radius of the green circle. We have estimated the radius distance of the blue circle to be <code>r = 20 pixels</code>.</p><p>So, for each pixel that is said to be the edge (with the image on the edge being colored both blue or green), then proceed to draw a new circle with the center at the position in the cell under consideration - call <code>x = position_row position</code> and <code>y = column_position</code>.</p><p>After obtaining the three values of <strong>x</strong>, <strong>y</strong> and <strong>r</strong>, the two values <strong>a</strong> and <strong>b</strong> can be deduced easily as follows:</p><div class="language-python"><pre><code>PI <span class="token operator">=</span> <span class="token number">3.14159265358979323846264338327950</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">361</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
    b <span class="token operator">=</span> y <span class="token operator">-</span> r <span class="token operator">*</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>t<span class="token operator">*</span>PI<span class="token operator">/</span><span class="token number">180</span><span class="token punctuation">)</span> 
    a <span class="token operator">=</span> x <span class="token operator">-</span> y <span class="token operator">*</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>t<span class="token operator">*</span>PI<span class="token operator">/</span><span class="token number">180</span><span class="token punctuation">)</span> 
    vote<span class="token punctuation">[</span>a<span class="token punctuation">,</span> b<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span> 
</code></pre></div><p>As above, we will select the threshold value and take the positions vote<code>[a, b] &gt; threshold</code>. The higher the threshold value is, the more likely it is to be a perfect circle. However, when the threshold value is too low, we are not sure that it is a circle (maybe even a small curve). In practice, it is virtually impossible to determine the radius in a fixed way, even if it is a human or a program with the best perception. So, when we practice in practice, we choose <strong>r</strong> in some interval <code>[m, n]</code>. This means that the voting array will increase from <strong>2D</strong> to <strong>3D</strong> as <code>[a, b, r]</code>.</p><h4 id="_6-references" tabindex="-1">6. References <a class="header-anchor" href="#_6-references" aria-hidden="true">#</a></h4><p>[1] L. Chandrasekar and G. Durga. Implementation of hough transform for image processing applications. In 2014 International Conference on Communication and Signal Processing, pages 843–847, 2014. <em>doi: 10.1109/ICCSP.2014.6949962</em>.</p><p>[2] J. Illingworth and J. Kittler. The adaptive hough transform. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-9(5):690–698, 1987. <em>doi: 10.1109/TPAMI.1987.4767964</em>.</p><p>[3] H. Kang, D.-Y. Kang, J.-S. Park, and S. W. Ha. Vgg19-based classification of amyloid pet image in patients with mci and ad. In 2018 International Conference on Computational Science and Computational Intelligence (CSCI), pages 1442–1443, 2018. <em>doi: 10.1109/CSCI46756.2018.00281</em>.</p><p>[4] T. Kaur and T. K. Gandhi. Automated brain image classification based on vgg-16 and transfer learning. In 2019 International Conference on Information Technology (ICIT), pages 94–98, 2019. <em>doi: 10.1109/ICIT48102.2019.00023</em>.</p><p>[5] S. Liu and W. Deng. Very deep convolutional neural network based image classification using small training sample size. In 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), pages 730–734, 2015. <em>doi: 10.1109/ACPR.2015.7486599</em>.</p><p>[6] Raghu, N. Sriraam, Y. Temel, S. V. Rao, and P. L. Kubben. A convolutional neural network based framework for classification of seizure types. In 2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pages 2547–2550, 2019. <em>doi: 10.1109/EMBC.2019.8857359</em>.</p><p>[7] M. Raman and H. Aggarwal. Study and comparison of various image edge detection techniques. <em>International Journal of Image Processing, 3, 03 2009</em>.</p><p>[8] M. Shaha and M. Pawar. Transfer learning for image classification. In 2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA), pages 656–660, 2018. <em>doi: 10.1109/ICECA.2018.8474802</em>.</p><p>[9] S. Singh and R. Singh. Comparison of various edge detection techniques. In 2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom), pages 393–396, 2015.</p><p>[10] L. Wen, X. Li, X. Li, and L. Gao. A new transfer learning based on vgg-19 network for fault diagnosis. In 2019 IEEE 23rd International Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 205–209, 2019. <em>doi: 10.1109/CSCWD.2019.8791884</em>.</p><p>[11] H. Ye, G. Shang, L. Wang, and M. Zheng. A new method based on hough transform for quick line and circle detection. In 2015 8th International Conference on Biomedical Engineering and Informatics (BMEI), pages 52–56, 2015. <em>doi: 10.1109/BMEI.2015.7401472</em>.</p><p>[12] R. M. Yousaf, H. A. Habib, H. Dawood, and S. Shafiq. A comparative study of various edge detection methods. In 2018 14th International Conference on Computational Intelligence and Security (CIS), pages 96–99, 2018. <em>doi: 10.1109/CIS2018.2018.00029</em>.</p><p>[13] Rezatofighi, S.H., Soltanian-Zadeh, H.: Automatic recognition of five types of white blood cells in peripheral blood. Computerized Medical Imaging and Graphics 35(4) (2011) 333--343.</p></div></div></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2"><div class="pt-8"><a class="link" href="/">← Back to the blog</a></div></footer></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2" style="display:inline-flex;justify-content:space-between;width:100%;"><div style="display:inline-flex;justify-content:space-between;width:100%;"><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500"> Previous Article </h2><div class="link"><a href="/posts/computer-vision-for-leukocyte-prediction.html">← Leukocyte classification to predict diseases (Part 1)</a></div></div><!----></div></footer></article></main></div></div>
    
    
    
  </body>
</html>