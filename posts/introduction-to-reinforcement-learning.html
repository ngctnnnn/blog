<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>An introduction to Reinforcement Learning | A technical blog</title>
    <meta name="description" content="The official blog">
    <link rel="stylesheet" href="/assets/style.a95e0a9f.css">
    
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <meta name="twitter:title" content="An introduction to Reinforcement Learning | A technical blog">
    <meta property="og:title" content="An introduction to Reinforcement Learning | A technical blog">
  </head>
  <body>
    <div id="app"><div class="antialiased"><div class="flex bg-white"><nav class="z-50 fixed w-full flex justify-center py-3 font-bold bg-white border border-fuchsia-600"><div class="w-full flex justify-between px-4 sm:px-6 xl:max-w-5xl xl:px-0"><a class="text-xl" href="/" aria-label="A technical blog"><img class="inline-block mr-2 spin-img" style="width:36px;height:31px;" alt="logo" src="/logo.png"><span class="hidden md:inline nav-text">A technical blog</span></a><div class="text-base text-gray-500 leading-5 space-x-6"><a class="hover:text-gray-700 text-xl" href="https://github.com/ngctnnnn/ngctnnnn.github.io" target="_blank" rel="noopener">About us</a></div></div></nav></div><main class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0 mt-14"><article class="xl:divide-y xl:divide-gray-200"><header class="pt-6 xl:pb-10 space-y-1 text-center"><dl><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-500"><time datetime="2022-02-13T12:00:00.000Z">February 13, 2022</time></dd></dl><h1 class="text-3xl leading-9 font-extrabold text-gray-900 tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">An introduction to Reinforcement Learning</h1></header><div class="divide-y xl:divide-y-0 divide-gray-200 xl:grid xl:grid-cols-4 xl:gap-x-10 pb-16 xl:pb-20" style="grid-template-rows:auto 1fr;"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200"><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center xl:block space-x-8 sm:space-x-12 xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img src="https://gravatar.com/avatar/6c1ae5231dcadf6b4297a6ddf6315478?s=80" alt="author image" class="w-10 h-10 rounded-full"><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900">Tan Ngoc Pham</dd><dt class="sr-only">Linkedin</dt><dd><a href="https://linkedin.com/in/ngctnnnn" target="_blank" rel="noopnener noreferrer" class="link">@ngctnnnn</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 xl:pb-0 xl:col-span-3 xl:row-span-2"><div style="position:relative;" class="prose max-w-none pt-10 pb-8"><div><p>Besides traditional data-based methods on Machine Learning, e.g. Clustering or Maximum Likelihood Estimation, Reinforcement Learning is a family in which tiny (or even no) data is required to do the training and testing phase. In this post, I would give a minor introduction on Reinforcement Learning, its basic concepts and methods.</p><hr><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h2><ul><li><ol><li><a href="#:~:text=1.%20What%20is%20Reinforcement%20Learning">Introduction</a></li></ol></li><li><ol start="2"><li><a href="#:~:text=2.%20Principal%20(mathematical)%20concepts%20of%20Reinforcement%20Learning">Principal (mathematical) concepts of Reinforcement Learning</a></li></ol><ul><li>a. <a href="#:~:text=a.%20Transition%20and%20Reward">Transition &amp; Reward</a></li><li>b. <a href="#:~:text=b.%20Policy">Policy </a></li><li>c. <a href="#:~:text=c.%20State-Value%20and%20Action-Value%20Function">State-Value &amp; Action-Value Function</a></li></ul></li><li><ol start="3"><li><a href="#:~:text=3.%20A%20literature%20on%20preliminary%20approaches%20in%25Reinforcement%20Learning">A literature on preliminary approaches in Reinforcement Learning</a></li></ol><ul><li>a. <a href="#:~:text=a.%20Dynamic%20Programming">Dynamic Programming</a></li><li>b. <a href="#:~:text=b.%20Q-Learning">Q-Learning</a></li></ul></li><li><ol start="4"><li><a href="#:~:text=4.%20Conclusions">Conclusions</a></li></ol></li></ul><hr><h2 id="_1-what-is-reinforcement-learning" tabindex="-1">1. What is Reinforcement Learning <a class="header-anchor" href="#_1-what-is-reinforcement-learning" aria-hidden="true">#</a></h2><p>Let say you want to help Pac-man on the game of Pac-man achieve <ins>the best score</ins> among <ins>different rounds and difficulties</ins>. However, you do not know surely how to play the game optimally and all you can do is to <ins>give Pac-man the instructions</ins> about the game only. On learning to find the best way to play the game with a maximum score, Pac-man has to <ins>play the game continuously, continuously, and continuously</ins>. That is the most basic understanding on what Reinforcement Learning looks like. <p align="center"><img width="50%" src="https://i.pinimg.com/originals/4a/f9/7b/4af97be15a1edae3f1b61cdb0a60d30a.gif " alt="illustration"></p></p><p>Mathematically, we could denote Pac-man as <strong>an agent</strong> and the game as <strong>the environment</strong>:</p><ul><li>One <ins>random strategy</ins> to play the Pac-man game as <strong>a policy</strong> (<code>œÄ ‚àà Œ†</code>).</li><li>One <ins>position of Pac-man at a specific time</ins> as <strong>a state</strong> (<code>s ‚àà S</code>).</li><li>An <ins>event that Pac-man moves</ins> forward (or backward of left or right) as <strong>an action</strong> (<b>a ‚àà A</b>).</li><li>A <ins>score that Pac-man received</ins> after taking an action as <strong>the reward</strong> (<code>r ‚àà R</code>). <p align="center"><img width="80%" src="https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png" alt="illustration"><div align="center"><b> Fig 1.</b> An illustration on Reinforcement Learning </div></p></li></ul><p>The ultimate goal of Reinforcement Learning is to find an optimal policy <b>œÄ<sup>*</sup></b> which gives the agent a maximum reward <b>r<sup><span>*</span></sup></b> on any diverse environments.</p><h2 id="_2-principal-mathematical-concepts-of-reinforcement-learning" tabindex="-1">2. Principal (mathematical) concepts of Reinforcement Learning <a class="header-anchor" href="#_2-principal-mathematical-concepts-of-reinforcement-learning" aria-hidden="true">#</a></h2><h3 id="a-transition-and-reward" tabindex="-1">a. Transition and Reward <a class="header-anchor" href="#a-transition-and-reward" aria-hidden="true">#</a></h3><p>Dummies&#39; reinforcement learning algorithms often require a model for the agent to learn and infer. A model is a descriptor in which our agent would contact with the environment and receive respective solution feedbacks. Two most contributing factors to receive accurate feedbacks on how the agent performs are <strong>transition probability function (P)</strong> and <strong>reward function (R)</strong>.</p><p>The <ins>transition probability function</ins> <strong>P</strong>, as it is called, records the probability of transitioning from state <strong>s</strong> to <strong>s‚Äô</strong> after taking action and get a reward <strong>r</strong> at time <strong>t</strong>.</p><p align="center"><b>P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) = <b>p</b>[<b>S</b><sub><b>t+1</b></sub> = <b>s&#39;</b>, <b>R</b><sub><b>t+1</b></sub> = <b>r</b> | <b>S</b><sub><b>t</b></sub> = <b>s</b>, <b>A</b><sub><b>t</b></sub> = <b>a</b>] </p><p>The equation could be understood as <em><strong>&quot;the transition probability of state s to s&#39; to get a reward r after taking action a equals to the conditional probability (p) of getting a reward of r after changing state from s to s&#39; and taking an action a&quot;</strong></em>.</p><p>And the <ins>transition state function</ins> could be defined as:</p><p align="center"><b>P<sub>ss&#39;, a</sub></b> = <b>P</b>(<b>s&#39; </b>|<b>s</b>,<b> a</b>) = <b><span>‚àë</span><sub>r ‚àà R</sub> P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) </p><p>The reward achieved on taking a new action <strong>a&#39;</strong> is demonstrated using <ins>a reward function</ins> <strong>R</strong> and equals to the expected reward after taking an action <strong>a</strong> in the state <strong>s</strong>.</p><p align="center"><b>R</b>(<b>s</b>, <b>a</b>) = <b>ùîº</b>[<b>R<sub>t + 1</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] = <b><span>‚àë</span><sub>r ‚àà R</sub></b>(<b>r</b>) x <b><span>‚àë</span><sub>s&#39; ‚àà S</sub>P</b>(<b>s&#39;, r</b> | <b>s, a</b>) </p><h3 id="b-policy" tabindex="-1">b. Policy <a class="header-anchor" href="#b-policy" aria-hidden="true">#</a></h3><p>A policy <strong>œÄ(<span>¬∑</span>)</strong> is a mapping from a state <strong>s</strong> to its corresponding action <strong>a</strong> according to a specific strategy to take a suitable move toward the current state of our agent. A policy could be deterministic or stochastic, as which demonstrated:</p><ul><li><b><ins>Stochastic policy</ins></b>: given a random state <strong>s</strong>, there is only one action <strong>a</strong> which always get the maximum reward <b>r<sup>*</sup></b>. <p align="center"><b>œÄ</b>(<b>s</b>) = <b>a</b></p></li><li><b><ins>Deterministic policy</ins></b>: given a random state <strong>s</strong>, the optimal action to get the highest reward is distributed over a probability distribution over all appropriate actions. <p align="center"><b>œÄ</b>(<b>a | s</b>) = <b>P</b><sub><b>œÄ</b></sub>[<b>A</b> = <b>a</b> | <b>S</b> = <b>s</b>] </p></li></ul><h3 id="c-state-value-and-action-value-function" tabindex="-1">c. State-Value and Action-Value Function <a class="header-anchor" href="#c-state-value-and-action-value-function" aria-hidden="true">#</a></h3><p>Value function, generally, is used as <ins>a metric to measure how good a state is and how high a reward we could achieve</ins> by staying in a state or taking an action.</p><p>Let denote <strong>G</strong> as the <b>total reward</b> that our agent would take, starting from time <strong>t</strong>: <p align="center"><b>G<sub>t</sub></b> = <b>R<sub>t+1</sub></b> + <b>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> = <b>0 ‚Üí</b><b><span>‚àû</span></b></sub><b>(R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><p>However, in reality, <ins>a reward would be more appetizing in the present than it is in the future</ins> (imagine which decision would you make if you are allowed to choose between receiving 100 bucks now or 100 bucks in 10 more years). It is, thus, a must to propose a coefficient of <strong>Œ≥ ‚àà [0; 1]</strong> to penalize rewards in the future times. The above equation would be rewritten as follows <p align="center"><b>G<sub>t</sub></b> = <b>Œ≥R<sub>t+1</sub></b> + <b>Œ≥<sup>2</sup>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> ‚àà [<b>0</b>;<b><span>‚àû</span></b>)</sub><b>(Œ≥<sup>k</sup> R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><h4 id="action-value-function" tabindex="-1">Action-Value function <a class="header-anchor" href="#action-value-function" aria-hidden="true">#</a></h4><p>The <ins>Action-Value value</ins> (Q-value) is the expected total reward return of a <strong>state-action pair</strong> at a given time <strong>t</strong> and following policy <b>œÄ</b>: <p align="center"><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] </p></p><h4 id="state-value-function" tabindex="-1">State-Value function <a class="header-anchor" href="#state-value-function" aria-hidden="true">#</a></h4><p>The <ins>State-Value</ins> value of a state <strong>s</strong> is the expected total reward return if we are in state <strong>s</strong> at time <strong>t</strong><p align="center"><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>] </p></p><p>On stochastic policy, <ins>the relationship between Action-Value and State-Value</ins> is demonstrated as: <div align="center"><p><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b><span>‚àë</span><sub>a ‚àà A</sub></b> [<b>œÄ</b>(<b>a</b> | <b>s</b>) * <b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p><p><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b><span>‚àë</span><sub>s&#39; ‚àà S</sub></b>[<b>P</b>(<b>s&#39;</b> | <b>s</b>, <b>a</b>) [<b>R</b>(<b>s</b>, <b>a</b>, <b>s&#39;</b>) + <b>Œ≥V<sub>œÄ</sub></b>(<b>s&#39;</b>)]] </p></div></p><h4 id="optimal-value-and-policy" tabindex="-1">Optimal value and policy <a class="header-anchor" href="#optimal-value-and-policy" aria-hidden="true">#</a></h4><p>By iterating around State-Value and Action-Value function, we could get the maximum return of the State-Value <p align="center"><b>V<sub><span>*</span></sub></b>(<b>s</b>) = <b>max<sub>œÄ</sub></b>{<b>V<sub>œÄ</sub></b>(<b>s</b>), <b>Q<sub><span>*</span></sub></b>(<b>s</b>, <b>a</b>)} = <b>max<sub>œÄ</sub></b>[<b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p></p><p>The optimal policy would be the argmax from the optimal policy from the aforementioned maximum return of the State-Value</p><p align="center"><b>œÄ<sub>*</sub></b> = argmax<sub><b>œÄ</b></sub>{<b>V<sub>œÄ</sub></b>(<b>s</b>) <b>œÄ<sub><span>*</span></sub></b>} </p><h2 id="_3-a-literature-on-preliminary-approaches-in-reinforcement-learning" tabindex="-1">3. A literature on preliminary approaches in Reinforcement Learning <a class="header-anchor" href="#_3-a-literature-on-preliminary-approaches-in-reinforcement-learning" aria-hidden="true">#</a></h2><p>In this section, I would drive you through the very initial solution for Reinforcement Learning</p><h3 id="a-dynamic-programming" tabindex="-1">a. Dynamic Programming <a class="header-anchor" href="#a-dynamic-programming" aria-hidden="true">#</a></h3><p>The Dynamic Programming is the first thought coming into Computer Scientists when Reinforcement Learning had just appeared as a successive of previous mathematics foundations such as Bellman Equations [<a href="https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8_71" target="_blank" rel="noopener noreferrer">Bellman (1959)</a>] and Markov Decision Processes [<a href="https://dl.acm.org/doi/10.5555/528623" target="_blank" rel="noopener noreferrer">Puterman (1994)</a>]. There are a great number of DP-based and DP-inspired algorithms to deal with Reinforcement Learning. There lie two most highlighted algorithms when mentioned about DP in RL which are <b><ins>Policy Iteration</ins></b> and <b><ins>Value Iteration</ins></b>.</p><p>Both algorithms are DP algorithms based on Bellman Equations which aimed to find an optimal policy <b>œÄ<sub>*</sub></b> in a Reinforcement Learning environment using <ins>one-step look-ahead strategy</ins>.</p><table><thead><tr><th style="text-align:center;">Policy Iteration</th><th style="text-align:center;">Value Iteration</th></tr></thead><tbody><tr><td style="text-align:center;">Faster</td><td style="text-align:center;">Slower</td></tr><tr><td style="text-align:center;">Fewer iterations</td><td style="text-align:center;">More iterations</td></tr><tr><td style="text-align:center;">Guaranteed to converge</td><td style="text-align:center;">Guaranteed to converge</td></tr><tr><td style="text-align:center;">Algorithm is more <b>sophisticated</b></td><td style="text-align:center;">Algorithm is more <b>trouble-free</b></td></tr><tr><td style="text-align:center;">Start with <b>a random policy</b></td><td style="text-align:center;">Start with <b>a random value function</b></td></tr><tr><td style="text-align:center;"><ins>Cheaper</ins> computational costs</td><td style="text-align:center;"><ins>More expensive</ins> computational costs</td></tr></tbody></table><p align="center"><b>Table 1. </b>A minor comparison between Policy Iteration and Value Iteration. </p><h4 id="policy-iteration" tabindex="-1">Policy Iteration <a class="header-anchor" href="#policy-iteration" aria-hidden="true">#</a></h4><p>In policy iteration, one fixed policy is utilized firstly. Then, the algorithm would traverse through iterations, on every iteration, the policy is evaluated and updated gradually until the convergence.</p><div class="language-python"><pre><code><span class="token keyword">def</span> <span class="token function">policy_iteration</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> max_iters<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Intialize a random policy</span>
    policy <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>policy<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">import</span> random
        policy<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        policy_stable <span class="token operator">=</span> <span class="token boolean">True</span> 

        <span class="token comment"># Evaluate the present policy</span>
        v_values <span class="token operator">=</span>  policy_evaluation<span class="token punctuation">(</span>env<span class="token punctuation">,</span> 
                                      max_iters<span class="token operator">=</span>max_iters<span class="token punctuation">,</span> 
                                      gamma<span class="token operator">=</span>gamma<span class="token punctuation">,</span> 
                                      policy<span class="token operator">=</span>policy<span class="token punctuation">)</span>

        <span class="token comment"># Update the policy</span>
        new_policy <span class="token operator">=</span> policy_improvement<span class="token punctuation">(</span>env<span class="token punctuation">,</span> 
                                        gamma<span class="token operator">=</span>gamma<span class="token punctuation">,</span> 
                                        v_val<span class="token operator">=</span>v_values<span class="token punctuation">,</span> 
                                        policy<span class="token operator">=</span>policy<span class="token punctuation">)</span>

        <span class="token comment"># Check convergence</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>array_equiv<span class="token punctuation">(</span>policy<span class="token punctuation">,</span> new_policy<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span> 
            policy_stable <span class="token operator">=</span> <span class="token boolean">True</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Converged at </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">-th iteration&#39;</span></span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            policy_stable <span class="token operator">=</span> <span class="token boolean">False</span>
        policy <span class="token operator">=</span> new_policy<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> policy
</code></pre></div><h4 id="value-iteration" tabindex="-1">Value Iteration <a class="header-anchor" href="#value-iteration" aria-hidden="true">#</a></h4><p>Value Iteration, on the other hand, begins with <ins>a selected value function</ins> and iterates over that value function. Value Iteration undergoes a heavier computing process by taking a maximum over the utility function for <ins>all possible actions</ins>. And subsequently, Value Iteration has a tendency to take more iterations before fully converged compared to the Policy one.</p><div class="language-python"><pre><code><span class="token keyword">def</span> <span class="token function">value_iteration</span><span class="token punctuation">(</span>env<span class="token punctuation">,</span> max_iters<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span class="token punctuation">:</span>
    v_values <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>max_iters<span class="token punctuation">)</span><span class="token punctuation">:</span>
        prev_v_values <span class="token operator">=</span> np<span class="token punctuation">.</span>copy<span class="token punctuation">(</span>v_values<span class="token punctuation">)</span>

        <span class="token comment"># Compute the value for state</span>
        <span class="token keyword">for</span> state <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>observation_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
            q_values <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token comment"># Compute the q-value for each action</span>
            <span class="token keyword">for</span> action <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span>action_space<span class="token punctuation">.</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>
                q_value <span class="token operator">=</span> <span class="token number">0</span>
                <span class="token comment"># Loop through each possible outcome</span>
                <span class="token keyword">for</span> prob<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> done <span class="token keyword">in</span> env<span class="token punctuation">.</span>P<span class="token punctuation">[</span>state<span class="token punctuation">]</span><span class="token punctuation">[</span>action<span class="token punctuation">]</span><span class="token punctuation">:</span>
                    q_value <span class="token operator">+=</span> prob <span class="token operator">*</span> <span class="token punctuation">(</span>reward <span class="token operator">+</span> gamma <span class="token operator">*</span> prev_v_values<span class="token punctuation">[</span>next_state<span class="token punctuation">]</span><span class="token punctuation">)</span>
                
                q_values<span class="token punctuation">.</span>append<span class="token punctuation">(</span>q_value<span class="token punctuation">)</span>
            
            <span class="token comment"># Select the best action</span>
            best_action <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>q_values<span class="token punctuation">)</span>
            v_values<span class="token punctuation">[</span>state<span class="token punctuation">]</span> <span class="token operator">=</span> q_values<span class="token punctuation">[</span>best_action<span class="token punctuation">]</span>
        
        <span class="token comment"># Check convergence</span>
        <span class="token keyword">if</span> np<span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>isclose<span class="token punctuation">(</span>v_values<span class="token punctuation">,</span> prev_v_values<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;Converged at </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">-th iteration.&#39;</span></span><span class="token punctuation">)</span>
            <span class="token keyword">break</span>
    
    <span class="token keyword">return</span> v_values
</code></pre></div><h3 id="b-q-learning" tabindex="-1">b. Q-Learning <a class="header-anchor" href="#b-q-learning" aria-hidden="true">#</a></h3><p>Q-Learning is <ins>a <b>model-free</b> reinforcement learning algorithm</ins> to learn the quality of action <strong>a</strong> to tell the agent which actions to execute at a certain state <strong>s</strong>. It does not require a model of a certain environment. It can handle dynamically environmental problems and achieve rewards without any adaptations. The development of Q-Learning is actually a breakthrough in the early days of reinforcement learning.</p><p>The interesting point in Q-Learning is the independence of the current policy to choose the second action <b>a<sub>t+1</sub></b>. It, essentially, estimates <b>Q<sub><span>*</span></sub></b> among the best Q-values, but it does not matter which action leads to this maximum Q-value and in the next step, this algorithm does not have to follow that policy.</p><div align="center"><img width="80%" src="/q-learning.png" alt="q-learning"><div align="center"> Pseudo code for Updating Q-Value <a href="https://github.com/DTA-UIT/Deep-Q-Network/blob/main/report/Deep%20Q%20Network.pdf">[Tan Ngoc Pham et al. (2020)]</a></div></div><h2 id="_4-conclusions" tabindex="-1">4. Conclusions <a class="header-anchor" href="#_4-conclusions" aria-hidden="true">#</a></h2><ul><li>Throughout this blog, I have brought to you <ins>the most preliminary concepts</ins> on what Reinforcement Learning looks like, its mathematical fundamental and some of the algorithms to deal with Reinforcement Learning problems.</li><li>Further construction in Reinforcement Learning and more intensive algorithms surrounding would be settled on later posts on this blog.</li></ul></div></div></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2"><div class="pt-8"><a class="link" href="/">‚Üê Back to the blog</a></div></footer></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2" style="display:inline-flex;justify-content:space-between;width:100%;"><div style="display:inline-flex;justify-content:space-between;width:100%;"><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500"> Previous Article </h2><div class="link"><a href="/posts/neural-architecture-search.html">‚Üê Neural Architecture Search</a></div></div><!----></div></footer></article></main></div></div>
    
    
    
  </body>
</html>