<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>An introduction to Reinforcement Learning | A technical blog</title>
    <meta name="description" content="The official blog">
    <link rel="stylesheet" href="/assets/style.a95e0a9f.css">
    
    <link rel="icon" type="image/x-icon" href="/favicon.ico">
    <meta name="twitter:title" content="An introduction to Reinforcement Learning | A technical blog">
    <meta property="og:title" content="An introduction to Reinforcement Learning | A technical blog">
  </head>
  <body>
    <div id="app"><div class="antialiased"><div class="flex bg-white"><nav class="z-50 fixed w-full flex justify-center py-3 font-bold bg-white border border-fuchsia-600"><div class="w-full flex justify-between px-4 sm:px-6 xl:max-w-5xl xl:px-0"><a class="text-xl" href="/" aria-label="A technical blog"><img class="inline-block mr-2 spin-img" style="width:36px;height:31px;" alt="logo" src="/logo.png"><span class="hidden md:inline nav-text">A technical blog</span></a><div class="text-base text-gray-500 leading-5 space-x-6"><a class="hover:text-gray-700 text-xl" href="https://github.com/ngctnnnn/ngctnnnn.github.io" target="_blank" rel="noopener">About us</a></div></div></nav></div><main class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0 mt-14"><article class="xl:divide-y xl:divide-gray-200"><header class="pt-6 xl:pb-10 space-y-1 text-center"><dl><dt class="sr-only">Published on</dt><dd class="text-base leading-6 font-medium text-gray-500"><time datetime="2022-02-13T12:00:00.000Z">February 13, 2022</time></dd></dl><h1 class="text-3xl leading-9 font-extrabold text-gray-900 tracking-tight sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">An introduction to Reinforcement Learning</h1></header><div class="divide-y xl:divide-y-0 divide-gray-200 xl:grid xl:grid-cols-4 xl:gap-x-10 pb-16 xl:pb-20" style="grid-template-rows:auto 1fr;"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200"><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center xl:block space-x-8 sm:space-x-12 xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img src="https://gravatar.com/avatar/6c1ae5231dcadf6b4297a6ddf6315478?s=80" alt="author image" class="w-10 h-10 rounded-full"><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900">Tan Ngoc Pham</dd><dt class="sr-only">Linkedin</dt><dd><a href="https://linkedin.com/in/ngctnnnn" target="_blank" rel="noopnener noreferrer" class="link">@ngctnnnn</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 xl:pb-0 xl:col-span-3 xl:row-span-2"><div style="position:relative;" class="prose max-w-none pt-10 pb-8"><div><p>Besides traditional data-based methods on Machine Learning, e.g. Clustering or Maximum Likelihood Estimation, Reinforcement Learning is a family in which tiny (or even no) data is required to do the training and testing phase. In this post, I would give a minor introduction on Reinforcement Learning, its basic concepts and methods.</p><hr><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="header-anchor" href="#table-of-contents" aria-hidden="true">#</a></h2><ul><li><ol><li><a href="https://ngctnnnn.github.io/posts/introduction-to-reinforcement-learning.html#1-What-is-Reinforcement-Learning" target="_blank" rel="noopener noreferrer">Introduction</a></li></ol></li><li><ol start="2"><li><a href="https://ngctnnnn.github.io/posts/introduction-to-reinforcement-learning.html" target="_blank" rel="noopener noreferrer">Principal (mathematical) concepts of Reinforcement Learning</a></li></ol></li><li><ol start="3"><li><a href="https://ngctnnnn.github.io/posts/introduction-to-reinforcement-learning.html" target="_blank" rel="noopener noreferrer">Conclusion</a></li></ol></li></ul><hr><h2 id="_1-what-is-reinforcement-learning" tabindex="-1">1. What is Reinforcement Learning <a class="header-anchor" href="#_1-what-is-reinforcement-learning" aria-hidden="true">#</a></h2><p>Let say you want to help Pac-man on the game of Pac-man achieve <ins>the best score</ins> among <ins>different rounds and difficulties</ins>. However, you do not know surely how to play the game optimally and all you can do is to <ins>give Pac-man the instructions</ins> about the game only. On learning to find the best way to play the game with a maximum score, Pac-man has to <ins>play the game continuously, continuously, and continuously</ins>. That is the most basic understanding on what Reinforcement Learning looks like. <p align="center"><img width="50%" src="https://i.pinimg.com/originals/4a/f9/7b/4af97be15a1edae3f1b61cdb0a60d30a.gif " alt="illustration"></p></p><p>Mathematically, we could denote Pac-man as <strong>an agent</strong> and the game as <strong>the environment</strong>:</p><ul><li>One <ins>random strategy</ins> to play the Pac-man game as <strong>a policy</strong> (<code>œÄ ‚àà Œ†</code>).</li><li>One <ins>position of Pac-man at a specific time</ins> as <strong>a state</strong> (<code>s ‚àà S</code>).</li><li>An <ins>event that Pac-man moves</ins> forward (or backward of left or right) as <strong>an action</strong> (<b>a ‚àà A</b>).</li><li>A <ins>score that Pac-man received</ins> after taking an action as <strong>the reward</strong> (<code>r ‚àà R</code>). <p align="center"><img width="80%" src="https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png" alt="illustration"><div align="center"><b> Fig 1.</b> An illustration on Reinforcement Learning </div></p></li></ul><p>The ultimate goal of Reinforcement Learning is to find an optimal policy <b>œÄ<sup>*</sup></b> which gives the agent a maximum reward <b>r<sup><span>*</span></sup></b> on any diverse environments.</p><h2 id="_2-principal-mathematical-concepts-of-reinforcement-learning" tabindex="-1">2. Principal (mathematical) concepts of Reinforcement Learning <a class="header-anchor" href="#_2-principal-mathematical-concepts-of-reinforcement-learning" aria-hidden="true">#</a></h2><h3 id="a-transition-and-reward" tabindex="-1">a. Transition and Reward <a class="header-anchor" href="#a-transition-and-reward" aria-hidden="true">#</a></h3><p>Dummies&#39; reinforcement learning algorithms often require a model for the agent to learn and infer. A model is a descriptor in which our agent would contact with the environment and receive respective solution feedbacks. Two most contributing factors to receive accurate feedbacks on how the agent performs are <strong>transition probability function (P)</strong> and <strong>reward function (R)</strong>.</p><p>The <ins>transition probability function</ins> <strong>P</strong>, as it is called, records the probability of transitioning from state <strong>s</strong> to <strong>s‚Äô</strong> after taking action and get a reward <strong>r</strong> at time <strong>t</strong>.</p><p align="center"><b>P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) = <b>p</b>[<b>S</b><sub><b>t+1</b></sub> = <b>s&#39;</b>, <b>R</b><sub><b>t+1</b></sub> = <b>r</b> | <b>S</b><sub><b>t</b></sub> = <b>s</b>, <b>A</b><sub><b>t</b></sub> = <b>a</b>] </p><p>The equation could be understood as <em><strong>&quot;the transition probability of state s to s&#39; to get a reward r after taking action a equals to the conditional probability (p) of getting a reward of r after changing state from s to s&#39; and taking an action a&quot;</strong></em>.</p><p>And the <ins>transition state function</ins> could be defined as:</p><p align="center"><b>P<sub>ss&#39;, a</sub></b> = <b>P</b>(<b>s&#39; </b>|<b>s</b>,<b> a</b>) = <b><span>‚àë</span><sub>r ‚àà R</sub> P</b>(<b>s&#39;</b>, <b>r</b> | <b>s</b>, <b>a</b>) </p><p>The reward achieved on taking a new action <strong>a&#39;</strong> is demonstrated using <ins>a reward function</ins> <strong>R</strong> and equals to the expected reward after taking an action <strong>a</strong> in the state <strong>s</strong>.</p><p align="center"><b>R</b>(<b>s</b>, <b>a</b>) = <b>ùîº</b>[<b>R<sub>t + 1</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] = <b><span>‚àë</span><sub>r ‚àà R</sub></b>(<b>r</b>) x <b><span>‚àë</span><sub>s&#39; ‚àà S</sub>P</b>(<b>s&#39;, r</b> | <b>s, a</b>) </p><h3 id="b-policy" tabindex="-1">b. Policy <a class="header-anchor" href="#b-policy" aria-hidden="true">#</a></h3><p>A policy <strong>œÄ(<span>¬∑</span>)</strong> is a mapping from a state <strong>s</strong> to its corresponding action <strong>a</strong> according to a specific strategy to take a suitable move toward the current state of our agent. A policy could be deterministic or stochastic, as which demonstrated:</p><ul><li><b><ins>Stochastic policy</ins></b>: given a random state <strong>s</strong>, there is only one action <strong>a</strong> which always get the maximum reward <b>r<sup>*</sup></b>. <p align="center"><b>œÄ</b>(<b>s</b>) = <b>a</b></p></li><li><b><ins>Deterministic policy</ins></b>: given a random state <strong>s</strong>, the optimal action to get the highest reward is distributed over a probability distribution over all appropriate actions. <p align="center"><b>œÄ</b>(<b>a | s</b>) = <b>P</b><sub><b>œÄ</b></sub>[<b>A</b> = <b>a</b> | <b>S</b> = <b>s</b>] </p></li></ul><h3 id="c-state-value-and-action-value-function" tabindex="-1">c. State-Value and Action-Value Function <a class="header-anchor" href="#c-state-value-and-action-value-function" aria-hidden="true">#</a></h3><p>Value function, generally, is used as <ins>a metric to measure how good a state is and how high a reward we could achieve</ins> by staying in a state or taking an action.</p><p>Let denote <strong>G</strong> as the <b>total reward</b> that our agent would take, starting from time <strong>t</strong>: <p align="center"><b>G<sub>t</sub></b> = <b>R<sub>t+1</sub></b> + <b>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> = <b>0 ‚Üí</b><b><span>‚àû</span></b></sub><b>(R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><p>However, in reality, <ins>a reward would be more appetizing in the present than it is in the future</ins> (imagine which decision would you make if you are allowed to choose between receiving 100 bucks now or 100 bucks in 10 more years). It is, thus, a must to propose a coefficient of <strong>Œ≥ ‚àà [0; 1]</strong> to penalize rewards in the future times. The above equation would be rewritten as follows <p align="center"><b>G<sub>t</sub></b> = <b>Œ≥R<sub>t+1</sub></b> + <b>Œ≥<sup>2</sup>R<sub>t+2</sub></b> + ... = <b><span>‚àë</span></b><sub><b>k</b> ‚àà [<b>0</b>;<b><span>‚àû</span></b>)</sub><b>(Œ≥<sup>k</sup> R</b><sub><b>t + k + 1</b></sub><b>)</b></p></p><h4 id="action-value-function" tabindex="-1">Action-Value function <a class="header-anchor" href="#action-value-function" aria-hidden="true">#</a></h4><p>The <ins>Action-Value value</ins> (Q-value) is the expected total reward return of a <strong>state-action pair</strong> at a given time <strong>t</strong> and following policy <b>œÄ</b>: <p align="center"><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>, <b>A<sub>t</sub></b> = <b>a</b>] </p></p><h4 id="state-value-function" tabindex="-1">State-Value function <a class="header-anchor" href="#state-value-function" aria-hidden="true">#</a></h4><p>The <ins>State-Value</ins> value of a state <strong>s</strong> is the expected total reward return if we are in state <strong>s</strong> at time <strong>t</strong><p align="center"><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b>ùîº<sub>œÄ</sub></b>[<b>G<sub>t</sub></b> | <b>S<sub>t</sub></b> = <b>s</b>] </p></p><p>On stochastic policy, <ins>the relationship between Action-Value and State-Value</ins> is demonstrated as: <div align="center"><p><b>V<sub>œÄ</sub></b>(<b>s</b>) = <b><span>‚àë</span><sub>a ‚àà A</sub></b> [<b>œÄ</b>(<b>a</b> | <b>s</b>) * <b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p><p><b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>) = <b><span>‚àë</span><sub>s&#39; ‚àà S</sub></b>[<b>P</b>(<b>s&#39;</b> | <b>s</b>, <b>a</b>) [<b>R</b>(<b>s</b>, <b>a</b>, <b>s&#39;</b>) + <b>Œ≥V<sub>œÄ</sub></b>(<b>s&#39;</b>)]] </p></div></p><h4 id="optimal-value-and-policy" tabindex="-1">Optimal value and policy <a class="header-anchor" href="#optimal-value-and-policy" aria-hidden="true">#</a></h4><p>By iterating around State-Value and Action-Value function, we could get the maximum return of the State-Value <p align="center"><b>V<sub><span>*</span></sub></b>(<b>s</b>) = <b>max<sub>œÄ</sub></b>{<b>V<sub>œÄ</sub></b>(<b>s</b>), <b>Q<sub><span>*</span></sub></b>(<b>s</b>, <b>a</b>)} = <b>max<sub>œÄ</sub></b>[<b>Q<sub>œÄ</sub></b>(<b>s</b>, <b>a</b>)] </p></p><p>The optimal policy would be the argmax from the optimal policy from the aforementioned maximum return of the State-Value</p><p align="center"><b>œÄ<sub>*</sub></b> = argmax<sub><b>œÄ</b></sub>{<b>V<sub>œÄ</sub></b>(<b>s</b>) <b>œÄ<sub><span>*</span></sub></b>} </p><h2 id="_3-conclusions" tabindex="-1">3. Conclusions <a class="header-anchor" href="#_3-conclusions" aria-hidden="true">#</a></h2><ul><li>Throughout this blog, I have brought to you <ins>the most preliminary concepts</ins> on what Reinforcement Learning looks like and its mathematical fundamental.</li><li>Further construction in Reinforcement Learning and the algorithms surrounding it would be settled on later posts on this blog.</li></ul></div></div></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2"><div class="pt-8"><a class="link" href="/">‚Üê Back to the blog</a></div></footer></div><footer class="text-sm font-medium leading-5 divide-y divide-gray-200 xl:col-start-1 xl:row-start-2" style="display:inline-flex;justify-content:space-between;width:100%;"><div style="display:inline-flex;justify-content:space-between;width:100%;"><div class="py-8"><h2 class="text-xs tracking-wide uppercase text-gray-500"> Previous Article </h2><div class="link"><a href="/posts/neural-architecture-search.html">‚Üê Neural Architecture Search</a></div></div><!----></div></footer></article></main></div></div>
    
    
    
  </body>
</html>